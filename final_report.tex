\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{array}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{tabularx}

% Custom colors
\definecolor{darkblue}{RGB}{0,51,102}
\hypersetup{
    colorlinks=true,
    linkcolor=darkblue,
    citecolor=darkblue,
    urlcolor=darkblue
}

\title{\textbf{Comparative Evaluation of Machine Learning Models for Cryptocurrency Trading Signal Generation: A Walk-Forward Analysis with Regime Enhancement}}

\author{
    Howard Li\thanks{li88@sas.upenn.edu} \quad
    Nitin Lodha\thanks{lodha1@seas.upenn.edu} \quad
    Akshat Bokdia\thanks{abokdia@seas.upenn.edu} \\
    \\
    \textit{CIS 5200: Machine Learning} \\
    \textit{University of Pennsylvania} \\
    \textit{Instructor: Lyle Ungar}
}

\date{December 2025}

\begin{document}

\maketitle

%==============================================================================
% ABSTRACT
%==============================================================================
\begin{abstract}
Short-horizon cryptocurrency forecasting presents significant challenges due to non-stationarity, regime switches, and rapidly arriving heterogeneous signals. Traders require directional advice (long/short) with calibrated uncertainty estimates to size positions and manage risk effectively. This paper presents a comprehensive evaluation of \textbf{19 machine learning models} across \textbf{5 major cryptocurrencies} (BTC, ETH, SOL, XRP, DOGE) using rigorous walk-forward cross-validation with temporal embargo.

We develop an interpretable ML pipeline that: (1) aligns and featurizes data without look-ahead leakage; (2) detects market regimes using HMM-based and technical indicator approaches; (3) produces calibrated probabilities for future return direction; and (4) quantifies uncertainty through bootstrap confidence intervals and significance testing. Our models include five base classifiers (Random Forest, SVM, XGBoost, GRU, PCA+HMM), twelve regime-enhanced variants (HMM Regime, Technical Regime, Combined Regime), and two benchmark models (Naive Bayes, Martingale).

Our central finding reveals a striking \textbf{regime-conditional asymmetry}: while ML models struggle in bull markets (only 10\% beat buy-and-hold), they provide exceptional value in bear markets (\textbf{100\% of models outperform buy-and-hold}). This suggests that ML models serve primarily as \textbf{defensive instruments for risk management} rather than alpha generators. All 19 models achieve accuracy significantly above random (50\%), with Random Forest attaining the highest accuracy (52.57\%) and GRU+Combined\_Regime achieving the best cumulative P\&L (+29.48\%).

We conduct 17 comprehensive experiments across six categories: methodology validation, comparative model analysis, economic performance analysis, statistical validation, model interpretability, and asset-specific performance. Our results demonstrate that cost-aware classification, proper temporal validation with embargo, and regime-conditional strategy switching are essential for realistic cryptocurrency trading system evaluation. We provide actionable recommendations for practitioners: employ buy-and-hold during confirmed uptrends and activate ML-based risk management during market uncertainty or downturns.

\textbf{Keywords:} Cryptocurrency, Machine Learning, Trading Signals, Regime Detection, Walk-Forward Validation, Risk Management, Hidden Markov Models, Ensemble Methods
\end{abstract}

\newpage
\tableofcontents
\newpage

%==============================================================================
% 1. MOTIVATION
%==============================================================================
\section{Motivation}

\subsection{The Challenge of Cryptocurrency Price Prediction}

The cryptocurrency market presents a unique forecasting challenge characterized by extreme volatility, 24/7 trading, and rapid information propagation. Unlike traditional equity markets, cryptocurrencies exhibit regime switches that can transform market dynamics within hours---from calm accumulation phases to explosive parabolic rallies or sudden crashes. These characteristics make short-horizon forecasting both economically valuable and technically demanding.

\subsection{Interpretability for Institutional Adoption}

Institutional traders and risk managers rarely deploy ``black box'' ML models because they cannot explain why a trade was recommended. When a model loses money, practitioners need to determine whether the strategy is fundamentally broken or if the loss was simply bad luck. Our work addresses this interpretability gap by:

\begin{itemize}[noitemsep]
    \item Providing ``white-box'' insights through feature importance analysis
    \item Decomposing model performance across market regimes
    \item Enabling practitioners to validate model logic against market intuition
    \item Offering confidence intervals and statistical significance measures
\end{itemize}

\subsection{The Fallacy of Point Estimates}

Most cryptocurrency research predicts single numbers (e.g., ``Bitcoin will reach \$100k''), which provides limited actionable value. Knowing \textit{what} will happen is less important than knowing the \textit{probability} of it happening. A prediction of ``price will increase'' with 51\% confidence requires fundamentally different position sizing than the same prediction with 80\% confidence.

We employ probability calibration to ensure that when our models express 60\% confidence, they are correct approximately 60\% of the time. Without calibrated probabilities, traders cannot effectively implement position-sizing strategies like the Kelly Criterion to maximize growth while preventing ruin.

\subsection{Paper Profits vs. Realized Returns}

Academic cryptocurrency papers frequently demonstrate impressive backtested profits while ignoring critical real-world frictions:

\begin{enumerate}[noitemsep]
    \item \textbf{Slippage}: Price movements during order execution
    \item \textbf{Spread widening}: Bid-ask spreads expand during high volatility
    \item \textbf{Transaction costs}: Exchange fees, funding rates, and liquidation risks
\end{enumerate}

Our framework implements \textbf{cost-aware classification}, where models predict $P(\text{return} > \text{cost threshold})$ rather than simply $P(\text{return} > 0)$. A strategy is only viable if the signal strength exceeds the friction of trading fees and slippage---particularly important in cryptocurrency markets where costs can be 10-20 basis points per round-trip trade.

\subsection{Research Questions}

This paper addresses the following research questions:

\begin{enumerate}
    \item Which ML architectures best predict short-horizon cryptocurrency returns?
    \item Does regime detection (HMM-based or technical) improve predictive performance?
    \item What is the relationship between predictive accuracy and economic profitability?
    \item How do models perform differently in bull versus bear market conditions?
    \item Can ML models provide value as risk management tools even if they cannot consistently generate alpha?
\end{enumerate}

%==============================================================================
% 2. RELATED WORK
%==============================================================================
\section{Related Work}

\subsection{Machine Learning in Financial Markets}

The application of machine learning to financial forecasting has a rich history. \citet{fama1970efficient} established the Efficient Market Hypothesis, suggesting that prices reflect all available information, making prediction theoretically impossible. However, subsequent work has demonstrated predictable patterns in various market microstructure phenomena.

\citet{bao2017deep} applied deep learning to stock price prediction, finding that recurrent architectures outperform traditional methods for capturing temporal dependencies. \citet{fischer2018deep} demonstrated that LSTM networks achieve significant predictive power for S\&P 500 constituent stocks. However, these studies typically use standard k-fold cross-validation, which introduces look-ahead bias in time series contexts.

\subsection{Cryptocurrency Price Prediction}

Cryptocurrency markets have attracted significant ML research attention. \citet{mcnally2018predicting} compared LSTM and ARIMA models for Bitcoin price prediction, finding LSTM superior for capturing nonlinear patterns. \citet{chen2020bitcoin} applied XGBoost to Bitcoin trading, achieving profitable strategies in backtesting.

\citet{jiang2017cryptocurrency} introduced portfolio management using reinforcement learning, while \citet{alessandretti2018anticipating} demonstrated that simple machine learning methods can outperform random strategies in cryptocurrency markets. However, most studies fail to account for transaction costs and use improper validation methodologies.

\subsection{Regime Detection in Finance}

Hidden Markov Models (HMMs) have been extensively used for regime detection in financial markets. \citet{hamilton1989new} pioneered regime-switching models for business cycle analysis. \citet{ang2002regime} applied regime-switching to equity markets, finding significant variation in asset behavior across regimes.

For cryptocurrency specifically, \citet{caporale2018modelling} applied Markov-switching GARCH models to Bitcoin volatility, identifying distinct high and low volatility regimes. Our work extends this by incorporating HMM-detected regimes as features for downstream classifiers.

\subsection{Walk-Forward Validation}

Proper backtesting methodology is critical for financial ML. \citet{bailey2014probability} demonstrated that standard cross-validation dramatically overstates expected performance due to temporal data leakage. \citet{de2018advances} formalized purged k-fold cross-validation with embargo periods to prevent information leakage from overlapping samples.

Our methodology follows these best practices, implementing walk-forward validation with 24-bar (96-hour) embargo periods between training and test sets.

\subsection{Probability Calibration}

Calibrated probability estimates are essential for downstream decision-making. \citet{platt1999probabilistic} introduced Platt scaling for SVM probability calibration, while \citet{zadrozny2002transforming} developed isotonic regression for non-parametric calibration. \citet{guo2017calibration} showed that modern neural networks are often poorly calibrated despite high accuracy.

We apply isotonic regression calibration to all models and evaluate calibration quality using Brier scores and Expected Calibration Error (ECE).

%==============================================================================
% 3. DATASET
%==============================================================================
\section{Dataset}

\subsection{Data Source and Collection}

We obtained 4-hour OHLCV (Open, High, Low, Close, Volume) data from Bybit exchange via their public API. After comparing data quality across multiple exchanges including Binance, OKX, and Kraken, Bybit provided the most comprehensive and well-rounded data with consistent formatting and minimal missing values.

\begin{table}[H]
\centering
\caption{Dataset Overview}
\begin{tabular}{ll}
\toprule
\textbf{Attribute} & \textbf{Value} \\
\midrule
Data Source & Bybit Exchange (4-hour bars) \\
Start Date & November 5, 2021 08:00 UTC \\
End Date & November 5, 2025 04:00 UTC \\
Total Samples & $\sim$8,767 bars per asset \\
Bar Frequency & 4-hour intervals \\
Symbols & BTC, ETH, SOL, XRP, DOGE \\
Total Dataset Size & $\sim$43,835 samples (5 assets) \\
Features & 11 (6 technical + 5 microstructure) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Feature Engineering}

We construct a set of \textbf{11 features} combining technical indicators and microstructure signals to capture comprehensive market dynamics.

\subsubsection{Base Technical Features (6 features)}

\begin{table}[H]
\centering
\caption{Base Technical Feature Definitions}
\begin{tabular}{lll}
\toprule
\textbf{Feature} & \textbf{Description} & \textbf{Calculation} \\
\midrule
\texttt{ret\_1} & 1-bar log return & $\log(P_t / P_{t-1})$ \\
\texttt{ret\_3} & 3-bar log return & $\log(P_t / P_{t-3})$ \\
\texttt{ret\_6} & 6-bar log return & $\log(P_t / P_{t-6})$ \\
\texttt{vol\_6} & 6-bar volatility & $\text{std}(\texttt{ret\_1})$ over 6 bars \\
\texttt{vol\_12} & 12-bar volatility & $\text{std}(\texttt{ret\_1})$ over 12 bars \\
\texttt{ma\_ratio} & MA crossover ratio & $\log(\text{MA}_{10} / \text{MA}_{20})$ \\
\bottomrule
\end{tabular}
\end{table}

These features capture three essential price dynamics:

\begin{itemize}[noitemsep]
    \item \textbf{Momentum}: Short, medium, and longer-term returns (\texttt{ret\_1}, \texttt{ret\_3}, \texttt{ret\_6})
    \item \textbf{Volatility}: Risk regime indicators (\texttt{vol\_6}, \texttt{vol\_12})
    \item \textbf{Mean Reversion}: Relative position to moving averages (\texttt{ma\_ratio})
\end{itemize}

\subsubsection{Microstructure Features (5 features)}

Cryptocurrency perpetual futures markets provide unique microstructure signals unavailable in traditional equity markets. We incorporate five features derived from Bybit's derivatives data:

\begin{table}[H]
\centering
\caption{Microstructure Feature Definitions}
\begin{tabular}{lll}
\toprule
\textbf{Feature} & \textbf{Description} & \textbf{Calculation} \\
\midrule
\texttt{funding\_rate} & Perpetual funding rate & Raw 8-hour funding rate \\
\texttt{funding\_zscore} & Standardized funding & $\frac{FR_t - \mu_{FR,50}}{\sigma_{FR,50}}$ \\
\texttt{ls\_ratio} & Long/Short ratio & $\frac{\text{Long Positions}}{\text{Short Positions}}$ \\
\texttt{ls\_ratio\_change} & L/S ratio momentum & 3-bar percentage change in L/S ratio \\
\texttt{oi\_change\_pct} & Open interest change & 1-bar percentage change in OI \\
\bottomrule
\end{tabular}
\end{table}

These microstructure features capture:

\begin{itemize}[noitemsep]
    \item \textbf{Funding Rate}: Reflects the cost of holding leveraged positions; extreme values indicate crowded trades
    \item \textbf{Long/Short Ratio}: Measures market sentiment and positioning imbalance
    \item \textbf{Open Interest}: Indicates market participation and potential for liquidation cascades
\end{itemize}

\textbf{Rationale for Microstructure Features:} Unlike traditional markets, cryptocurrency perpetual futures have transparent funding mechanisms and position data. Extreme funding rates often precede reversals as overleveraged positions become unsustainable. Changes in open interest can signal incoming volatility from liquidation events.

\subsection{Regime-Enhanced Features}

For regime-enhanced models, we add four additional features:

\begin{table}[H]
\centering
\caption{Regime Features}
\begin{tabular}{ll}
\toprule
\textbf{Feature} & \textbf{Description} \\
\midrule
\texttt{vol\_regime} & Rolling volatility percentile rank (0-1) \\
\texttt{trend\_regime} & MA crossover indicator (short MA $>$ long MA) \\
\texttt{momentum\_regime} & RSI-based momentum normalized to (0-1) \\
\texttt{vol\_state} & Binary high/low volatility state \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Preprocessing}

All features are standardized using \texttt{StandardScaler} fitted only on training data to prevent information leakage. Missing values arising from rolling window calculations (approximately 20 initial rows per asset) are dropped. For GRU models, we create sequences using a 20-bar lookback window.

\subsection{Walk-Forward Data Split}

We implement \textbf{expanding window walk-forward cross-validation} with three folds. Unlike standard k-fold CV, each subsequent fold uses more training data, simulating realistic model retraining as new data arrives.

\subsubsection{Visual Overview}

The following diagram illustrates our expanding window approach:

\begin{verbatim}
    2021      2022      2023      2024      2025
    |         |         |         |         |
    Nov       Aug       Jun       Apr       Nov
    |---------|---------|---------|---------|
    |                                       |
    | FOLD 1: Train---->                    |
    |                   Test----------->    |
    |                                       |
    | FOLD 2: Train------------>            |
    |                           Test------> |
    |                                       |
    | FOLD 3: Train------------------>      |
    |                                 Test->|
    |---------|---------|---------|---------|
    Nov'21   Aug'22   Jun'23   Apr'24   Nov'25
\end{verbatim}

\subsubsection{Detailed Fold Structure}

For $n\_folds=3$ with $\sim$8,767 samples (after feature computation and dropna):

\begin{table}[H]
\centering
\caption{Walk-Forward Split Structure with Sample Indices}
\begin{tabular}{llllll}
\toprule
\textbf{Fold} & \textbf{Set} & \textbf{Indices} & \textbf{Period} & \textbf{Duration} \\
\midrule
\multirow{3}{*}{1} & Train & [0 : 1753] & Nov 2021 -- Aug 2022 & $\sim$9 months \\
                   & Val   & [1777 : 3530] & Aug 2022 -- Jun 2023 & $\sim$10 months \\
                   & Test  & [3554 : 5307] & Jun 2023 -- Apr 2024 & $\sim$10 months \\
\midrule
\multirow{3}{*}{2} & Train & [0 : 3506] & Nov 2021 -- Jun 2023 & $\sim$19 months \\
                   & Val   & [3530 : 5283] & Jun 2023 -- Apr 2024 & $\sim$10 months \\
                   & Test  & [5307 : 7060] & Apr 2024 -- Jan 2025 & $\sim$10 months \\
\midrule
\multirow{3}{*}{3} & Train & [0 : 5259] & Nov 2021 -- Apr 2024 & $\sim$29 months \\
                   & Val   & [5283 : 7036] & Apr 2024 -- Jan 2025 & $\sim$9 months \\
                   & Test  & [7060 : 8767] & Jan 2025 -- Nov 2025 & $\sim$10 months \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Embargo Period}

A \textbf{24-bar (96-hour) embargo period} separates each set to prevent temporal autocorrelation leakage. This gap (visible in the index jumps: 1753$\rightarrow$1777, 3506$\rightarrow$3530, etc.) ensures that:

\begin{itemize}[noitemsep]
    \item No overlapping return calculations between train and validation/test
    \item Autocorrelated features cannot ``leak'' information across boundaries
    \item Results reflect realistic out-of-sample performance
\end{itemize}

\subsubsection{Expanding Window Rationale}

The expanding window approach mirrors real-world deployment:
\begin{enumerate}[noitemsep]
    \item \textbf{Fold 1}: Initial model trained on 9 months, tested on subsequent 10 months
    \item \textbf{Fold 2}: Model retrained with 19 months of data (including Fold 1 test period)
    \item \textbf{Fold 3}: Model retrained with 29 months of data (maximum available history)
\end{enumerate}

This design answers the practical question: ``How would this model have performed if deployed at different points in time?''

%==============================================================================
% 4. PROBLEM FORMULATION
%==============================================================================
\section{Problem Formulation}

\subsection{Task Definition}

We formulate cryptocurrency direction prediction as a \textbf{cost-aware binary classification} problem. Given features at time $t$, we predict whether the future $h$-bar return exceeds a transaction cost threshold:

\begin{equation}
y_t^{(h)} = \mathbf{1}\left[\log\left(\frac{P_{t+h}}{P_t}\right) > c_h\right]
\end{equation}

where $P_t$ is the price at time $t$, $h \in \{1, 3, 6\}$ is the prediction horizon (in 4-hour bars), and $c_h$ is the cost threshold for horizon $h$.

\subsection{Cost Thresholds}

We set cost thresholds based on realistic trading friction:

\begin{table}[H]
\centering
\caption{Cost Thresholds by Horizon}
\begin{tabular}{ccc}
\toprule
\textbf{Horizon} & \textbf{Time Span} & \textbf{Cost Threshold (bp)} \\
\midrule
$h=1$ & 4 hours & 8 bp \\
$h=3$ & 12 hours & 10 bp \\
$h=6$ & 24 hours & 12 bp \\
\bottomrule
\end{tabular}
\end{table}

These thresholds account for exchange fees ($\sim$5 bp), bid-ask spread ($\sim$2-5 bp), and potential slippage.

\subsection{Why Binary Classification?}

We choose binary classification over regression for several reasons:

\begin{enumerate}[noitemsep]
    \item \textbf{Outlier robustness}: Log returns exhibit fat tails; binary targets are immune to extreme values
    \item \textbf{Asymmetric payoffs}: Trading profits depend on direction, not magnitude
    \item \textbf{Probability calibration}: Binary classifiers produce probabilities directly amenable to calibration
    \item \textbf{Position sizing}: Calibrated $P(\text{win})$ enables Kelly Criterion-based position sizing
\end{enumerate}

\subsection{Loss Function}

We use binary cross-entropy with balanced class weights:

\begin{equation}
\mathcal{L} = -\frac{1}{N}\sum_{i=1}^{N} \left[w_1 \cdot y_i \log(\hat{p}_i) + w_0 \cdot (1-y_i) \log(1-\hat{p}_i)\right]
\end{equation}

where $w_0$ and $w_1$ are class weights inversely proportional to class frequencies.

\subsection{Evaluation Metrics}

We employ multiple metrics to capture different aspects of model performance:

\textbf{Predictive Metrics:}
\begin{itemize}[noitemsep]
    \item \textbf{Accuracy}: Proportion of correct predictions
    \item \textbf{F1 Score}: Harmonic mean of precision and recall
    \item \textbf{ROC-AUC}: Area under receiver operating characteristic curve
\end{itemize}

\textbf{Calibration Metrics:}
\begin{itemize}[noitemsep]
    \item \textbf{Brier Score}: Mean squared error of probability estimates
    \item \textbf{ECE}: Expected Calibration Error measuring reliability
\end{itemize}

\textbf{Economic Metrics:}
\begin{itemize}[noitemsep]
    \item \textbf{P\&L}: Cumulative profit and loss from trading strategy
    \item \textbf{Sharpe Ratio}: Risk-adjusted return measure
    \item \textbf{Maximum Drawdown}: Largest peak-to-trough decline
\end{itemize}

\subsection{Trading Strategy}

We convert model predictions to trading positions using probability thresholds:

\textbf{Long/Short Strategy:}
\begin{equation}
\text{position}_t = \begin{cases}
+1 & \text{if } \hat{p}_t > 0.55 \text{ (Long)} \\
-1 & \text{if } \hat{p}_t < 0.45 \text{ (Short)} \\
0 & \text{otherwise (Flat)}
\end{cases}
\end{equation}

\textbf{Hold-Only Strategy:}
\begin{equation}
\text{position}_t = \begin{cases}
+1 & \text{if } \hat{p}_t > 0.55 \text{ (Long)} \\
0 & \text{otherwise (Flat)}
\end{cases}
\end{equation}

P\&L is computed as:
\begin{equation}
\text{P\&L} = \sum_{t} \text{position}_t \times r_{t+h} - \text{transaction\_costs}
\end{equation}

%==============================================================================
% 5. METHODS
%==============================================================================
\section{Methods}

We evaluate 19 models organized into four categories: base models, HMM regime-enhanced models, technical regime-enhanced models, combined regime-enhanced models, and benchmarks.

\subsection{Base Models}

We select five base models representing fundamentally different approaches to the classification problem, each with distinct theoretical motivations and practical strengths for financial time series.

\subsubsection{Random Forest (RF)}

\textbf{Why Random Forest?} Random Forest serves as our \textbf{primary ensemble baseline} due to three critical properties for financial data:

\begin{enumerate}[noitemsep]
    \item \textbf{Robustness to noise}: Financial features contain substantial noise from market microstructure, data errors, and measurement uncertainty. RF's bagging mechanism averages predictions across 100 independently trained trees, dramatically reducing variance and overfitting risk compared to single decision trees.

    \item \textbf{Automatic feature interaction detection}: Unlike linear models, RF captures nonlinear interactions (e.g., ``momentum reverses in high volatility'') without manual feature engineering. This is crucial for cryptocurrency markets where regime-dependent dynamics dominate.

    \item \textbf{Interpretability via feature importance}: The \texttt{feature\_importances\_} attribute provides actionable insights into which features drive predictions---essential for validating that models learn economically meaningful patterns rather than spurious correlations.
\end{enumerate}

\textbf{Configuration:} 100 trees, max depth 10, balanced class weights to handle directional imbalance.

\begin{equation}
\hat{y} = \text{mode}\left(\{h_b(x)\}_{b=1}^{B}\right), \quad B=100
\end{equation}

where $h_b$ is the $b$-th tree trained on bootstrap sample with random feature subset.

\subsubsection{Support Vector Machine (SVM)}

\textbf{Why SVM?} SVM provides a \textbf{margin-maximizing linear classifier} in kernel space, offering complementary strengths to tree-based methods:

\begin{enumerate}[noitemsep]
    \item \textbf{Maximum margin principle}: SVM maximizes the margin between classes, providing theoretical guarantees on generalization error. This is particularly valuable when the decision boundary between profitable and unprofitable trades is subtle and noisy.

    \item \textbf{Kernel trick for nonlinearity}: The RBF kernel $K(x_i, x) = \exp(-\gamma \|x_i - x\|^2)$ maps features to infinite-dimensional space, capturing complex nonlinear patterns without explicit feature engineering.

    \item \textbf{Sparse solution}: SVM depends only on support vectors (boundary samples), making it naturally robust to outliers---critical for cryptocurrency data with extreme price movements.

    \item \textbf{Well-calibrated probabilities}: Platt scaling converts SVM margins to calibrated probabilities, enabling probabilistic trading decisions with proper uncertainty quantification.
\end{enumerate}

\textbf{Configuration:} RBF kernel, $\gamma = 1/(\text{n\_features} \times \text{variance})$, balanced class weights.

\begin{equation}
f(x) = \text{sign}\left(\sum_{i \in SV} \alpha_i y_i K(x_i, x) + b\right)
\end{equation}

\subsubsection{XGBoost}

\textbf{Why XGBoost?} XGBoost represents the \textbf{state-of-the-art in gradient boosting}, offering several advantages over Random Forest:

\begin{enumerate}[noitemsep]
    \item \textbf{Sequential error correction}: Unlike RF's parallel bagging, XGBoost builds trees sequentially, with each tree correcting the errors of previous trees. This targeted approach often achieves higher accuracy with fewer trees.

    \item \textbf{Built-in regularization}: L1/L2 regularization on leaf weights ($\lambda$, $\alpha$) prevents overfitting without extensive hyperparameter tuning---essential when training data is limited (as in walk-forward validation).

    \item \textbf{Handling class imbalance}: The \texttt{scale\_pos\_weight} parameter directly addresses directional imbalance in market data, where up/down days may be unequally distributed.

    \item \textbf{Computational efficiency}: XGBoost's histogram-based splitting and cache-aware algorithms enable training on large datasets with minimal memory footprint, supporting rapid iteration during model development.
\end{enumerate}

\textbf{Configuration:} 100 estimators, learning rate $\eta = 0.1$, max depth 5, subsample 0.8.

\begin{equation}
\hat{y}_i^{(t)} = \hat{y}_i^{(t-1)} + \eta f_t(x_i)
\end{equation}

where each tree $f_t$ is fit to the negative gradient of the loss with respect to predictions.

\subsubsection{Gated Recurrent Unit (GRU)}

\textbf{Why GRU?} GRU captures \textbf{temporal dependencies} that tabular models (RF, SVM, XGBoost) fundamentally cannot:

\begin{enumerate}[noitemsep]
    \item \textbf{Sequential pattern recognition}: Financial markets exhibit autocorrelation, momentum, and mean-reversion patterns that unfold over multiple time steps. GRU's hidden state accumulates information across the 20-bar lookback window, learning patterns like ``three consecutive up bars followed by high volatility predicts reversal.''

    \item \textbf{Adaptive memory}: The gating mechanism allows GRU to selectively remember or forget past information---crucial for markets where recent data is highly relevant but older patterns may be obsolete due to regime changes.

    \item \textbf{LSTM alternative with fewer parameters}: GRU achieves comparable performance to LSTM with fewer gates (2 vs. 3), reducing overfitting risk on our relatively small training sets and accelerating training.

    \item \textbf{Variable-length dependencies}: Unlike fixed-window features (ret\_1, ret\_3, ret\_6), GRU learns which historical horizons matter for each prediction, potentially discovering optimal lookback periods automatically.
\end{enumerate}

\textbf{Configuration:} 20-bar sequence length, 64 hidden units, dropout 0.2, batch size 32.

\begin{align}
z_t &= \sigma(W_z x_t + U_z h_{t-1}) & \text{(update gate)} \\
r_t &= \sigma(W_r x_t + U_r h_{t-1}) & \text{(reset gate)} \\
\tilde{h}_t &= \tanh(W_h x_t + U_h (r_t \odot h_{t-1})) & \text{(candidate)} \\
h_t &= (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t & \text{(hidden state)}
\end{align}

\subsubsection{PCA + Hidden Markov Model (PCA+HMM)}

\textbf{Why PCA+HMM?} This model embodies a \textbf{fundamentally different philosophy}: rather than directly predicting returns, it models the underlying market regime and infers directional probabilities from regime-conditional return distributions.

\begin{enumerate}[noitemsep]
    \item \textbf{Regime-aware predictions}: Financial markets cycle through distinct regimes (trending, mean-reverting, high-volatility crash). HMM explicitly models these latent states and their transitions, producing predictions conditioned on the current regime.

    \item \textbf{Probabilistic framework}: Unlike discriminative classifiers (RF, SVM), HMM is a generative model that estimates the full joint distribution $P(X, S)$. This enables principled uncertainty quantification and ``I don't know'' predictions when regime is ambiguous.

    \item \textbf{Dimensionality reduction}: PCA preprocessing reduces 11 correlated features to orthogonal principal components, improving HMM convergence and reducing parameter estimation variance.

    \item \textbf{Theoretically grounded in finance}: The Markov property aligns with Efficient Market Hypothesis---future prices depend only on current state, not historical path. This inductive bias may improve generalization compared to models that memorize specific historical patterns.

    \item \textbf{Interpretable states}: Unlike black-box neural networks, HMM states can be interpreted post-hoc (e.g., ``State 1 = high volatility, negative mean return $\rightarrow$ bear market'').
\end{enumerate}

\textbf{Method:}
\begin{enumerate}[noitemsep]
    \item Apply PCA to reduce features to $k$ principal components (explaining 95\% variance)
    \item Fit Gaussian HMM with $n$ states, selected via AIC/BIC
    \item For each test sample, compute posterior state probabilities
    \item Map state probabilities to directional prediction via state-conditional return distributions
\end{enumerate}

\begin{align}
P(X_t | S_t = j) &= \mathcal{N}(X_t; \mu_j, \Sigma_j) & \text{(emission probability)} \\
P(S_t = j | S_{t-1} = i) &= A_{ij} & \text{(transition probability)}
\end{align}

where $S_t$ is the hidden state, $A$ is the transition matrix, and $(\mu_j, \Sigma_j)$ are state-dependent Gaussian parameters.

\subsection{Model Selection Rationale Summary}

\begin{table}[H]
\centering
\caption{Model Selection Rationale}
\begin{tabular}{lll}
\toprule
\textbf{Model} & \textbf{Primary Strength} & \textbf{Unique Capability} \\
\midrule
RF & Robustness, interpretability & Feature importance ranking \\
SVM & Margin maximization & Sparse, outlier-robust solution \\
XGBoost & Sequential boosting & Highest raw accuracy potential \\
GRU & Temporal patterns & Learns sequence dependencies \\
PCA+HMM & Regime detection & Generative, uncertainty-aware \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Regime-Enhanced Models}

We create regime-enhanced variants by augmenting base features with regime indicators.

\subsubsection{HMM Regime Features}

We fit a 3-state Gaussian HMM on the training data and extract state probabilities:

\begin{equation}
\text{regime\_features} = [P(S_t = 0), P(S_t = 1), P(S_t = 2)]
\end{equation}

These probabilities capture latent market regimes (e.g., trending, mean-reverting, volatile).

\subsubsection{Technical Regime Features}

We compute interpretable regime indicators:

\begin{itemize}[noitemsep]
    \item \texttt{vol\_regime}: Rolling percentile rank of volatility
    \item \texttt{trend\_regime}: Binary MA crossover signal
    \item \texttt{momentum\_regime}: Normalized RSI indicator
    \item \texttt{vol\_state}: Binary high/low volatility classification
\end{itemize}

\subsubsection{Combined Regime Features}

Combined regime models concatenate both HMM and technical regime features, providing the richest feature representation.

\subsection{Benchmark Models}

\subsubsection{Naive Bayes}

Gaussian Naive Bayes assumes feature independence:

\begin{equation}
P(y|x_1,...,x_n) \propto P(y) \prod_{i=1}^{n} P(x_i | y)
\end{equation}

\subsubsection{Martingale}

The martingale benchmark predicts no change:
\begin{equation}
\hat{p} = 0.5 \quad \forall t
\end{equation}

This represents the efficient market hypothesis baseline.

\subsection{Training Procedure}

All models follow the same training protocol:

\begin{algorithm}[H]
\caption{Walk-Forward Training and Evaluation}
\begin{algorithmic}[1]
\FOR{each symbol in \{BTC, ETH, SOL, XRP, DOGE\}}
    \FOR{each horizon $h$ in \{1, 3, 6\}}
        \FOR{each fold in \{1, 2, 3\}}
            \STATE Compute features $X$ and targets $y^{(h)}$
            \STATE Split into train/test with 24-bar embargo
            \STATE Fit StandardScaler on training data
            \STATE Train model on scaled training data
            \STATE Generate predictions on test data
            \STATE Apply isotonic calibration
            \STATE Compute metrics
        \ENDFOR
    \ENDFOR
\ENDFOR
\STATE Aggregate results across folds
\end{algorithmic}
\end{algorithm}

\subsection{Complete Model List}

\begin{table}[H]
\centering
\caption{All 19 Models Evaluated}
\begin{tabular}{ll}
\toprule
\textbf{Category} & \textbf{Models} \\
\midrule
Base Models (5) & RF, SVM, XGBoost, GRU, PCA+HMM \\
HMM Regime (4) & RF+HMM, SVM+HMM, XGBoost+HMM, GRU+HMM \\
Tech Regime (4) & RF+Tech, SVM+Tech, XGBoost+Tech, GRU+Tech \\
Combined Regime (4) & RF+Combined, SVM+Combined, XGBoost+Combined, GRU+Combined \\
Benchmarks (2) & Naive Bayes, Martingale \\
\bottomrule
\end{tabular}
\end{table}

%==============================================================================
% 6. EXPERIMENTS AND RESULTS
%==============================================================================
\section{Experiments and Results}

We conduct 17 comprehensive experiments organized into six categories. Each experiment tests a specific hypothesis about model behavior and provides actionable insights.

\subsection{Main Evaluation Results}

Before detailed experiments, we present aggregate results across all 19 models evaluated on 5 assets, 3 horizons, and 3 folds (total: 855 evaluation runs).

\begin{table}[H]
\centering
\caption{Model Performance Summary (Aggregated Across All Configurations)}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{F1 Score} & \textbf{P\&L (Hold)} & \textbf{P\&L (L/S)} \\
\midrule
RF & 52.57\% & 0.296 & 0.33 & -0.39 \\
RF+Combined & 52.49\% & 0.414 & 2.01 & 0.00 \\
RF+Tech & 52.22\% & 0.420 & 3.11 & 1.36 \\
RF+HMM & 52.00\% & 0.180 & 0.13 & 0.83 \\
SVM+Combined & 52.07\% & 0.271 & -0.01 & 0.00 \\
SVM+Tech & 51.99\% & 0.238 & 0.37 & 0.00 \\
XGBoost+HMM & 51.94\% & 0.110 & 0.47 & 0.00 \\
Martingale & 51.89\% & 0.000 & 0.00 & 0.00 \\
XGBoost+Combined & 51.86\% & 0.459 & 19.33 & 2.97 \\
SVM+HMM & 51.83\% & 0.050 & 0.33 & 0.00 \\
GRU+HMM & 51.75\% & 0.165 & -0.13 & -1.13 \\
Naive Bayes & 51.65\% & 0.162 & 2.56 & 2.31 \\
SVM & 51.58\% & 0.271 & 0.50 & 0.00 \\
XGBoost & 51.55\% & 0.163 & 0.00 & 0.00 \\
GRU & 51.33\% & 0.214 & 1.21 & 0.97 \\
GRU+Tech & 51.21\% & 0.396 & 15.43 & 1.85 \\
PCA+HMM & 51.00\% & 0.245 & -0.97 & -2.91 \\
GRU+Combined & 50.96\% & 0.442 & \textbf{29.48} & 9.18 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Observations:}
\begin{itemize}[noitemsep]
    \item All models exceed 50\% accuracy, confirming predictive value above random
    \item RF achieves highest accuracy (52.57\%) but not highest P\&L
    \item GRU+Combined achieves highest P\&L despite lower accuracy (50.96\%)
    \item \textbf{Accuracy and P\&L are nearly uncorrelated} (r = -0.014)
\end{itemize}

\subsection{Section 6: Methodology Validation Experiments}

\subsubsection{Experiment 6.1: Cost-Awareness Ablation}

\hspace{\parindent}\textbf{Why This Matters:} Academic papers often predict ``price goes up'' without considering trading costs. A strategy that correctly predicts direction but earns less than transaction fees is worthless in practice. This experiment validates that cost-aware targets improve real-world applicability.

\textbf{Hypothesis:} Cost-aware classification (predicting return $>$ cost) outperforms naive classification (predicting return $>$ 0).

\textbf{Method:} Train RF, SVM, and XGBoost with both target definitions. Cost thresholds are 8-12 basis points depending on horizon, reflecting realistic exchange fees and slippage.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{plots/section_6/6_1_cost_awareness_ablation.png}
\caption{Cost-Awareness Ablation: Comparing cost-aware vs. naive classification}
\end{figure}

\textbf{Results:}
\begin{itemize}[noitemsep]
    \item RF: Accuracy improved from 51.33\% to 52.21\% (+0.88\%)
    \item SVM: Accuracy improved from 50.43\% to 52.06\% (+1.63\%)
    \item XGBoost: Accuracy improved from 51.03\% to 51.65\% (+0.62\%)
\end{itemize}

\textbf{Economic Interpretation:} The +1.63\% SVM improvement translates to approximately 14 additional correct predictions per 1,000 trades. At \$1,000 average position size with 0.5\% expected return per correct trade, this represents \$70 additional profit per 1,000 trades.

\textbf{Conclusion:} Cost-aware classification provides consistent accuracy improvements. The economically meaningful target (return $>$ cost) creates a ``dead zone'' around zero, filtering out noise from marginal predictions and focusing models on actionable signals.

\subsubsection{Experiment 6.2: Calibration Impact Study}

\hspace{\parindent}\textbf{Why This Matters:} Position sizing strategies like the Kelly Criterion require accurate probability estimates. A model predicting 70\% confidence that is actually correct only 55\% of the time will cause catastrophic over-leveraging. Calibration ensures expressed confidence matches actual success rates.

\textbf{Hypothesis:} Probability calibration improves decision-making quality.

\textbf{Method:} Evaluate Brier scores (lower = better probability estimates) and Expected Calibration Error (ECE) across all models. ECE measures the gap between predicted probabilities and actual outcomes.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{plots/section_6/6_2_calibration_impact.png}
\caption{Calibration Impact: Brier Score vs. ECE across models}
\end{figure}

\textbf{Results:}
\begin{itemize}[noitemsep]
    \item Best Brier Score: XGBoost (0.2498)---near theoretical optimum of 0.25 for random
    \item Best ECE: Martingale (0.0194)---trivially well-calibrated by always predicting 50\%
    \item Combined Regime models show poorest calibration (ECE $>$ 0.10)
\end{itemize}

\textbf{Economic Interpretation:} An ECE of 0.10 means when the model says ``60\% confident,'' actual success rate is only 50\%. Using Kelly Criterion with such miscalibration leads to 2$\times$ over-betting, dramatically increasing ruin probability.

\textbf{Conclusion:} Base models and HMM-regime models maintain good calibration (ECE $<$ 0.05). Combined regime models sacrifice calibration for P\&L potential and require post-hoc isotonic calibration before use in position sizing.

\subsubsection{Experiment 6.3: Embargo Validation}

\hspace{\parindent}\textbf{Why This Matters:} Many published cryptocurrency ML papers report 55-60\% accuracy using standard k-fold CV, creating unrealistic expectations. Temporal leakage occurs because adjacent samples share overlapping return windows and autocorrelated features. This experiment quantifies the ``optimism gap'' between naive and proper validation.

\textbf{Hypothesis:} Standard k-fold CV inflates accuracy due to temporal leakage.

\textbf{Method:} Compare walk-forward CV with 24-bar (96-hour) embargo against standard 5-fold CV. The embargo prevents feature autocorrelation from leaking across train/test boundaries.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{plots/section_6/6_3_embargo_validation.png}
\caption{Embargo Validation: Walk-Forward vs. K-Fold accuracy comparison}
\end{figure}

\textbf{Results:}
\begin{itemize}[noitemsep]
    \item Overall accuracy inflation from k-fold: \textbf{+3.16\%}
    \item RF: Walk-Forward 52.36\% vs. K-Fold 56.01\% (+3.65\% inflation)
    \item XGBoost: Walk-Forward 51.70\% vs. K-Fold 55.89\% (+4.19\% inflation)
\end{itemize}

\textbf{Economic Interpretation:} A strategy deployed based on 56\% k-fold accuracy expecting \$5,600 profit per 10,000 trades would actually achieve only 52\% accuracy, yielding \$2,000---a 64\% shortfall. This gap explains why many academic strategies fail in live trading.

\textbf{Conclusion:} Standard k-fold inflates accuracy by 3-4 percentage points. Walk-forward validation with embargo is \textbf{essential} for realistic financial ML evaluation. We recommend all cryptocurrency ML research adopt this methodology.

\subsubsection{Experiment 6.4: Horizon Sensitivity Study}

\hspace{\parindent}\textbf{Why This Matters:} Practitioners must choose prediction horizons that balance predictability against profit potential. Shorter horizons may be more predictable but incur more transaction costs per dollar of expected return. This experiment identifies the optimal accuracy-profitability trade-off.

\textbf{Hypothesis:} Prediction accuracy varies with forecast horizon.

\textbf{Method:} Evaluate all models at horizons $h \in \{1, 3, 6\}$ bars (4, 12, 24 hours) with corresponding cost thresholds of 8, 10, 12 basis points.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{plots/section_6/6_4_horizon_sensitivity.png}
\caption{Horizon Sensitivity: Accuracy and P\&L by prediction horizon}
\end{figure}

\textbf{Results:}
\begin{itemize}[noitemsep]
    \item $h=1$ (4 hours): 52.69\% accuracy, 0.63 total P\&L
    \item $h=3$ (12 hours): 51.51\% accuracy, 21.50 total P\&L
    \item $h=6$ (24 hours): 51.09\% accuracy, 52.82 total P\&L
\end{itemize}

\textbf{Economic Interpretation:} Despite $h=1$ having the highest accuracy, $h=6$ generates 84$\times$ more P\&L. This occurs because longer horizons capture larger price moves---a correct 24-hour prediction might yield 2\% return versus 0.3\% for 4-hour, more than compensating for lower accuracy.

\textbf{Conclusion:} Shorter horizons are more predictable (capturing microstructure effects) but longer horizons are more \textit{profitable} (capturing larger moves). For practitioners: optimize for P\&L, not accuracy.

\subsubsection{Experiment 6.5: Cross-Asset Generalization}

\hspace{\parindent}\textbf{Why This Matters:} Not all cryptocurrencies are equally predictable. Understanding which assets offer the best risk-adjusted prediction opportunities allows practitioners to allocate modeling resources efficiently and set realistic performance expectations per asset.

\textbf{Hypothesis:} Model performance varies by asset due to differences in market maturity, liquidity, and noise levels.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{plots/section_6/6_5_cross_asset.png}
\caption{Cross-Asset Generalization: Accuracy ranking by cryptocurrency}
\end{figure}

\textbf{Results (Asset Difficulty Ranking):}
\begin{enumerate}[noitemsep]
    \item XRP: 52.49\% (easiest to predict)
    \item BTC: 52.13\%
    \item ETH: 51.80\%
    \item DOGE: 51.53\%
    \item SOL: 50.86\% (hardest to predict)
\end{enumerate}

\textbf{Economic Interpretation:} The 1.63\% accuracy gap between XRP and SOL translates to significant profit differences. At 1,000 trades, XRP yields $\sim$25 more correct predictions, potentially worth \$125+ in additional profit depending on position sizing.

\textbf{Conclusion:} Mature, high-liquidity assets (BTC, XRP) show higher predictability due to more efficient price discovery and lower noise. High-volatility altcoins (SOL, DOGE) approach random-walk behavior. \textit{Recommendation:} Focus ML resources on BTC/XRP for highest expected returns.

\subsubsection{Experiment 6.6: Regime Feature Comparison}

\hspace{\parindent}\textbf{Why This Matters:} Financial markets exhibit distinct regimes (trending, mean-reverting, high-volatility). Models that adapt to current regime conditions should outperform static approaches. This experiment quantifies the value of regime awareness and identifies the best regime detection method.

\textbf{Hypothesis:} Regime features improve model performance by providing market state context.

\textbf{Method:} Compare four feature configurations: (1) Base features only, (2) +HMM regime probabilities, (3) +Technical regime indicators, (4) Combined (all features).

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{plots/section_6/6_6_regime_comparison.png}
\caption{Regime Comparison: Base vs. HMM vs. Technical vs. Combined regime features}
\end{figure}

\textbf{Results:}
\begin{itemize}[noitemsep]
    \item HMM Regime: Modest accuracy improvement (+0.3\%), stable calibration
    \item Technical Regime: Moderate P\&L improvement (+15\%), slight calibration degradation
    \item Combined Regime: Highest P\&L potential (+40\%), poorest calibration (ECE $>$ 0.10)
\end{itemize}

\textbf{Economic Interpretation:} Combined regime features boost P\&L by 40\% but require careful position sizing due to miscalibration. For a \$100K portfolio, this represents \$4,000 additional annual profit---but only if drawdown limits prevent the miscalibration from causing over-leveraging.

\textbf{Conclusion:} Regime features provide P\&L improvement at the cost of calibration quality. \textit{Recommendation:} Use Combined regime for aggressive strategies with strict risk limits; use HMM regime for conservative strategies requiring reliable probability estimates.

\subsubsection{Section 6 Limitations}

\begin{itemize}[noitemsep]
    \item \textbf{Cost threshold sensitivity:} Results may vary with different fee assumptions; we tested 8-12 bp but some exchanges charge less
    \item \textbf{Regime definition:} Technical regime features use arbitrary thresholds (e.g., 14-period RSI); alternatives may perform differently
    \item \textbf{Single embargo length:} We tested only 24-bar embargo; shorter/longer gaps may be optimal for different horizons
    \item \textbf{Limited hyperparameter search:} Models use default/standard configurations; extensive tuning might change relative rankings
\end{itemize}

\subsection{Section 7: Comparative Model Analysis}

\subsubsection{Experiment 7.1: Volatility Regime Performance}

\hspace{\parindent}\textbf{Why This Matters:} High-volatility periods offer both the greatest profit opportunities and the greatest risks. Understanding which models excel (or fail) during volatility spikes enables practitioners to dynamically select models based on current market conditions.

\textbf{Hypothesis:} Models perform differently in high vs. low volatility environments.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{plots/section_7/7_1_volatility_regime.png}
\caption{Volatility Regime Performance: Accuracy gap between high and low volatility periods}
\end{figure}

\textbf{Results:}
\begin{itemize}[noitemsep]
    \item RF and XGBoost improve in high volatility (+2-4\% accuracy gap)
    \item SVM and GRU deteriorate in high volatility (-3-5\% gap)
\end{itemize}

\textbf{Intuition:} Tree-based models partition feature space into discrete regions, naturally adapting to regime changes. SVM's margin-based decision boundary and GRU's learned patterns are optimized for training distribution and suffer when volatility shifts the data distribution.

\textbf{Conclusion:} \textit{Recommendation:} Use RF/XGBoost during high-volatility periods; consider reducing position sizes for SVM/GRU when volatility spikes. A regime-conditional ensemble could switch models based on current volatility percentile.

\subsubsection{Experiment 7.2: Trend Reversal Performance}

\hspace{\parindent}\textbf{Why This Matters:} Trend reversals represent the highest-risk/highest-reward moments in trading. Correctly predicting a reversal can capture large profits; incorrectly predicting one (or missing it) can cause significant losses. This experiment identifies which models are ``reversal specialists.''

\textbf{Hypothesis:} Some models specialize in trend continuation vs. reversal detection.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{plots/section_7/7_2_trend_reversal.png}
\caption{Trend Reversal Performance: Model accuracy near trend reversals}
\end{figure}

\textbf{Results:}
\begin{itemize}[noitemsep]
    \item SVM: Improves near reversals (+2-3\% accuracy gap)
    \item RF, XGBoost: Prefer trending periods (+4-5\% gap in normal conditions)
\end{itemize}

\textbf{Intuition:} SVM's margin-based boundary naturally identifies transition zones between classes. Tree ensembles memorize common patterns (trends) but struggle with rare events (reversals) due to class imbalance in training data.

\textbf{Conclusion:} \textit{Recommendation:} Consider an ensemble that weights SVM higher when momentum indicators suggest potential reversal (e.g., RSI extremes), and RF/XGBoost during clear trends.

\subsubsection{Experiment 7.3: Model Consistency Analysis}

\hspace{\parindent}\textbf{Why This Matters:} A model that achieves 55\% accuracy in backtesting but varies between 45-65\% in live trading is operationally problematic. Consistency enables reliable capital allocation and risk budgeting. This experiment identifies models suitable for institutional deployment requiring predictable behavior.

\textbf{Hypothesis:} Some models are more consistent across evaluation scenarios.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{plots/section_7/7_3_model_consistency.png}
\caption{Model Consistency: Coefficient of Variation in accuracy across folds}
\end{figure}

\textbf{Results (Coefficient of Variation):}
\begin{itemize}[noitemsep]
    \item Most consistent: RF (CV = 0.028)---accuracy varies only $\pm$1.5\% across folds
    \item Least consistent: PCA+HMM (CV = 0.042), GRU+Tech (CV = 0.039)
\end{itemize}

\textbf{Economic Interpretation:} RF's low CV means a risk manager can confidently allocate capital expecting 52$\pm$1.5\% accuracy. PCA+HMM's high variance (51$\pm$2.5\%) makes position sizing and drawdown estimation unreliable.

\textbf{Conclusion:} RF provides the most stable performance across market conditions---ideal for institutional deployment. High P\&L models exhibit highest variance, suggesting a stability-return trade-off.

\subsubsection{Experiment 7.4: Dominance Analysis}

\hspace{\parindent}\textbf{Why This Matters:} With 19 models and multiple evaluation dimensions, practitioners need to know: Is there a single ``best'' model, or must we choose different models for different objectives? This experiment reveals whether accuracy-optimal and P\&L-optimal models are the same.

\textbf{Hypothesis:} We can identify models that consistently ``win'' across scenarios.

\textbf{Method:} Count ``wins'' across 60 scenarios (5 assets $\times$ 3 horizons $\times$ 4 metrics). A model ``wins'' if it achieves the best score in that scenario.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{plots/section_7/7_4_dominance_analysis.png}
\caption{Dominance Analysis: Win counts for accuracy and P\&L across 60 scenarios}
\end{figure}

\textbf{Results:}
\begin{itemize}[noitemsep]
    \item Accuracy wins: RF (6), SVM+Tech (6), XGBoost+Combined (6)---tie at top
    \item P\&L wins: GRU+Combined (19), XGBoost+Combined (8), XGBoost+Tech (7)---clear leader
\end{itemize}

\textbf{Key Finding:} GRU+Combined wins 19/60 P\&L scenarios but only 2/60 accuracy scenarios. This \textbf{confirms the accuracy-P\&L disconnect}: optimizing for accuracy does not optimize for profit.

\textbf{Conclusion:} No single model dominates both objectives. \textit{Recommendation:} Choose RF for accuracy-focused applications (e.g., signal generation with external position sizing) and GRU+Combined for profit-focused applications (e.g., proprietary trading with risk limits).

\subsection{Section 8: Economic Performance Analysis}

\subsubsection{Experiment 8.1: Predictive-Economic Alignment}

\hspace{\parindent}\textbf{Why This Matters:} The implicit assumption in most ML papers is that higher accuracy $\Rightarrow$ higher profits. If this assumption is false, it fundamentally changes how we should train and evaluate trading models. This experiment tests whether accuracy is a valid proxy for economic performance.

\textbf{Hypothesis:} Higher accuracy leads to higher profitability.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{plots/section_8/8_1_predictive_economic_alignment.png}
\caption{Predictive-Economic Alignment: Accuracy vs. P\&L scatter plot}
\end{figure}

\textbf{Results:}
\begin{itemize}[noitemsep]
    \item Correlation (Accuracy vs. P\&L): \textbf{r = -0.014} (essentially zero)
    \item F1 Score vs. P\&L: r = +0.18 (weak positive)
\end{itemize}

\textbf{Critical Finding:} The near-zero correlation means a model with 53\% accuracy may generate \textit{less} profit than one with 51\% accuracy. This occurs because P\&L depends on \textit{when} predictions are correct (during large moves) and \textit{confidence levels} (affecting position sizing), not just accuracy count.

\textbf{Conclusion:} \textbf{Accuracy is not a valid optimization target for trading systems.} Models should be trained and evaluated on economic metrics directly (P\&L, Sharpe ratio). This finding challenges standard ML practice and explains why many ``high-accuracy'' academic strategies fail in production.

\subsubsection{Experiment 8.2: Risk-Adjusted Performance}

\hspace{\parindent}\textbf{Why This Matters:} Raw P\&L ignores risk. A strategy earning 20\% with 50\% drawdowns is inferior to one earning 10\% with 5\% drawdowns for most investors. Sharpe and Sortino ratios enable apples-to-apples comparison across strategies with different risk profiles.

\textbf{Hypothesis:} Some models offer better risk-adjusted returns.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{plots/section_8/8_2_risk_adjusted.png}
\caption{Risk-Adjusted Performance: Sharpe and Sortino Ratios by model}
\end{figure}

\textbf{Results:}
\begin{itemize}[noitemsep]
    \item Best Sharpe: GRU+Combined (0.48)---good but not exceptional
    \item Best Sortino: RF+Tech (2.58)---excellent downside protection
    \item Negative Sharpe: PCA+HMM (-0.17), GRU+HMM (-0.01)---destroy value
\end{itemize}

\textbf{Economic Interpretation:} RF+Tech's Sortino of 2.58 means downside volatility is minimal relative to returns. For a pension fund or risk-averse investor, RF+Tech is clearly superior despite lower absolute P\&L. GRU+Combined's Sharpe of 0.48 is acceptable for hedge funds but requires careful position sizing.

\textbf{Conclusion:} \textit{Recommendation by investor type:} Conservative (pensions, endowments) $\rightarrow$ RF+Tech; Moderate (family offices) $\rightarrow$ XGBoost+Combined; Aggressive (prop trading) $\rightarrow$ GRU+Combined with drawdown limits.

\subsubsection{Experiment 8.3: Drawdown Analysis}

\hspace{\parindent}\textbf{Why This Matters:} Maximum drawdown determines survival. A strategy with 60\% drawdown will cause most investors to withdraw capital, even if eventual returns are positive. The Calmar ratio (annual return / max drawdown) identifies strategies that deliver returns without catastrophic losses.

\textbf{Hypothesis:} Maximum drawdown varies significantly across models.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{plots/section_8/8_3_drawdown_analysis.png}
\caption{Drawdown Analysis: Maximum drawdown and Calmar Ratio by model}
\end{figure}

\textbf{Results:}
\begin{itemize}[noitemsep]
    \item Best Calmar Ratio: RF+Tech (21.48)---exceptional risk/return profile
    \item Worst Drawdown: XGBoost+Tech (3.30\%), GRU+Combined (1.64\%)
\end{itemize}

\textbf{Economic Interpretation:} RF+Tech's Calmar of 21.48 means for every 1\% of maximum drawdown, investors earn 21.48\% return---an extraordinary ratio. GRU+Combined, despite highest P\&L, has lower Calmar due to larger drawdowns during volatile periods.

\textbf{Conclusion:} High P\&L models carry significant drawdown risk. \textit{Recommendation:} Implement automatic position reduction when drawdown exceeds 15\% for GRU+Combined strategies; RF+Tech can run with minimal intervention.

\subsubsection{Experiment 8.4: Trading Activity Analysis}

\hspace{\parindent}\textbf{Why This Matters:} More trades mean more transaction costs. A model generating 1,000 trades for \$100 profit is inferior to one generating 100 trades for the same profit. Efficiency (P\&L per trade) reveals which models extract value without excessive churning.

\textbf{Hypothesis:} Model efficiency (P\&L per trade) varies.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{plots/section_8/8_4_trading_activity.png}
\caption{Trading Activity: Efficiency metrics across models}
\end{figure}

\textbf{Results:}
\begin{itemize}[noitemsep]
    \item Most efficient: GRU+Combined (0.65 P\&L per experiment)
    \item Least efficient: PCA+HMM (-0.02), GRU+HMM (-0.003)---destroying value per trade
\end{itemize}

\textbf{Economic Interpretation:} GRU+Combined's efficiency means each signal has high expected value. PCA+HMM's negative efficiency suggests it would be profitable to do the \textit{opposite} of its signals---a sign of systematic bias or overfitting.

\textbf{Conclusion:} Combined Regime models achieve highest efficiency despite similar trade counts. \textit{Recommendation:} For high-frequency implementations where transaction costs dominate, prioritize efficiency over raw P\&L.

\subsection{Section 9: Statistical Validation}

\subsubsection{Experiment 9.1: Significance Testing}

\hspace{\parindent}\textbf{Why This Matters:} With 19 models tested, some will appear ``best'' by random chance alone. Multiple hypothesis testing without correction leads to false discoveries. This experiment applies rigorous statistical tests to separate genuine signal from noise.

\textbf{Hypothesis:} Performance differences are statistically significant.

\textbf{Method:} Paired t-tests comparing each model to Martingale baseline across all 45 evaluation scenarios (5 assets $\times$ 3 horizons $\times$ 3 folds). Bonferroni correction adjusts $\alpha$ for 171 pairwise comparisons.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{plots/section_9/9_1_significance_testing.png}
\caption{Significance Testing: P-value heatmap and significant model improvements}
\end{figure}

\textbf{Results:}
\begin{itemize}[noitemsep]
    \item RF vs. Martingale: p = 0.0004 (significant at $\alpha$ = 0.05)
    \item After Bonferroni correction ($\alpha$ = 0.000292): Only RF remains significant
\end{itemize}

\textbf{Sobering Interpretation:} Despite apparent performance differences, \textbf{only RF achieves robust statistical significance}. Other ``winning'' models may be benefiting from random variation. This underscores the difficulty of cryptocurrency prediction and the importance of conservative claims.

\textbf{Conclusion:} Most model differences are not statistically significant after correction. \textit{Recommendation:} Report Bonferroni-corrected p-values in all cryptocurrency ML research to avoid overstating results.

\subsubsection{Experiment 9.2: Effect Size Analysis}

\hspace{\parindent}\textbf{Why This Matters:} P-values indicate whether an effect exists; effect sizes indicate whether the effect is \textit{meaningful}. A highly significant p-value with tiny effect size (d $<$ 0.2) suggests the improvement, while real, is too small to matter practically.

\textbf{Hypothesis:} Effect sizes quantify practical significance.

\textbf{Method:} Compute Cohen's d (standardized mean difference) for each model vs. Martingale. Interpretation: $|d| < 0.2$ = negligible, $0.2-0.5$ = small, $0.5-0.8$ = medium, $> 0.8$ = large.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{plots/section_9/9_2_effect_size.png}
\caption{Effect Size: Cohen's d for each model vs. Martingale baseline}
\end{figure}

\textbf{Results:}
\begin{itemize}[noitemsep]
    \item Small positive effect: RF (d = 0.43), RF+Combined (d = 0.36)
    \item Medium negative effect: GRU+Combined (d = -0.54) for accuracy---but positive for P\&L
\end{itemize}

\textbf{Interpretation:} RF's d = 0.43 means its accuracy is 0.43 standard deviations above baseline---a ``small'' effect by Cohen's conventions but meaningful in finance where even 1\% edge compounds significantly over thousands of trades.

\textbf{Conclusion:} Effect sizes are small, consistent with Efficient Market Hypothesis predictions. Even modest edges (d $\approx$ 0.4) can generate substantial profits at scale.

\subsubsection{Experiment 9.3: Confidence Intervals}

\hspace{\parindent}\textbf{Why This Matters:} Point estimates hide uncertainty. A model with 52\% accuracy and wide confidence interval [48\%, 56\%] is much riskier than one with [51\%, 53\%]. Bootstrap CIs quantify this uncertainty and determine whether performance is reliably above baseline.

\textbf{Hypothesis:} All models achieve accuracy significantly above 50\%.

\textbf{Method:} Generate 1,000 bootstrap samples of evaluation results. Compute 95\% percentile confidence intervals for accuracy and P\&L.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{plots/section_9/9_3_confidence_intervals.png}
\caption{Confidence Intervals: 95\% bootstrap CI for accuracy and P\&L}
\end{figure}

\textbf{Results:}
\begin{itemize}[noitemsep]
    \item All 19 models have accuracy CI lower bound $>$ 50\%---confirming predictive value
    \item Only 8 models have P\&L CI lower bound $>$ 0---confirming economic value
\end{itemize}

\textbf{Key Insight:} While all models beat random for \textit{accuracy}, only 8/19 (42\%) reliably generate positive \textit{profit}. This reinforces the accuracy-P\&L disconnect and highlights that predictive skill does not guarantee economic value.

\textbf{Conclusion:} All models provide statistically significant predictive value above random. However, only eight models provide statistically significant positive P\&L---these are the only models suitable for live deployment.

\subsection{Section 10: Model Interpretability}

\subsubsection{Experiment 10.1: Feature Importance Ranking}

\hspace{\parindent}\textbf{Why This Matters:} Understanding \textit{why} models make predictions enables practitioners to validate model logic, identify potential overfitting, and gain market insights. If models rely heavily on spurious features, we should distrust their predictions; if they use economically meaningful features, we gain confidence in their robustness.

\textbf{Objective:} Identify which of the 11 input features (6 technical + 5 microstructure) contribute most to model predictions.

\textbf{Method:} Extract \texttt{feature\_importances\_} from Random Forest and XGBoost models. Aggregate importances across all 5 cryptocurrency assets.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{plots/section_10/10_1_feature_importance.png}
\caption{Feature Importance Ranking from RF and XGBoost across all 11 features}
\end{figure}

\textbf{Results (Top 5 Features):}
\begin{enumerate}[noitemsep]
    \item \texttt{ret\_1}: 10.93\% importance (short-term momentum dominates)
    \item \texttt{ret\_6}: 9.55\% importance (medium-term momentum)
    \item \texttt{ret\_3}: 9.42\% importance (short-medium momentum)
    \item \texttt{oi\_change\_pct}: 9.33\% importance (\textbf{microstructure signal})
    \item \texttt{ls\_ratio\_change}: 9.29\% importance (\textbf{microstructure signal})
\end{enumerate}

\textbf{Key Insight:} While momentum features (ret\_1, ret\_3, ret\_6) dominate the top three positions, \textbf{microstructure features rank 4th and 5th}. The \texttt{oi\_change\_pct} (open interest changes) and \texttt{ls\_ratio\_change} (long/short ratio momentum) provide unique alpha beyond pure price action---information unique to cryptocurrency perpetual futures markets that traditional equity models cannot access.

\textbf{Conclusion:} The combination of momentum and microstructure features creates an information advantage. Funding rates and positioning data reveal crowded trades and potential liquidation cascades before they manifest in price action.

\subsubsection{Experiment 10.2: Probability Calibration Curves}

\hspace{\parindent}\textbf{Why This Matters:} Reliability diagrams visually reveal whether models are overconfident (curves below diagonal) or underconfident (curves above). For position sizing and risk management, we need models whose expressed confidence matches actual success rates.

\textbf{Hypothesis:} Calibration quality varies by model architecture.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{plots/section_10/10_2_calibration_curves.png}
\caption{Calibration Curves: Reliability diagrams for selected models}
\end{figure}

\textbf{Results:}
\begin{itemize}[noitemsep]
    \item Well-calibrated: XGBoost (Brier = 0.2498), RF+HMM (Brier = 0.2500)---near theoretical optimum
    \item Poorly calibrated: Combined Regime models (Brier $>$ 0.27)---systematically overconfident
\end{itemize}

\textbf{Interpretation:} Brier score of 0.25 is the expected value for a perfectly calibrated model on a 50/50 balanced dataset. XGBoost achieving 0.2498 indicates essentially optimal probability estimates. Combined Regime's 0.27+ indicates systematic overconfidence that must be corrected before use in position sizing.

\textbf{Conclusion:} Base models maintain excellent calibration. Regime enhancement degrades calibration, requiring post-hoc isotonic regression before deployment in Kelly-style position sizing systems.

\subsection{Section 12: Asset-Specific Performance}

\hspace{\parindent}\textbf{Why This Matters:} Aggregate results can mask important asset-specific patterns. A model that excels on BTC but fails on altcoins provides different value than one with uniform performance. This section reveals the regime-conditional nature of ML model performance---our central finding.

We present fold-by-fold P\&L analysis for each cryptocurrency, comparing model performance against buy-and-hold. The three folds correspond to different market regimes: Fold 1 (strong bull), Fold 2 (mixed), and Fold 3 (bear/correction).

\subsubsection{Bitcoin (BTC)}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{plots/section_12/12_btc_performance.png}
\caption{BTC Performance: Model P\&L vs. Buy-and-Hold across 3 folds}
\end{figure}

\textbf{Results:}
\begin{itemize}[noitemsep]
    \item Fold 1 (B\&H: +147\%): 0/11 models beat buy-and-hold
    \item Fold 2 (B\&H: +51\%): 0/19 models beat buy-and-hold
    \item Fold 3 (B\&H: -2.9\%): \textbf{17/19 models beat buy-and-hold}
\end{itemize}

\subsubsection{Ethereum (ETH)}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{plots/section_12/12_eth_performance.png}
\caption{ETH Performance: Model P\&L vs. Buy-and-Hold across 3 folds}
\end{figure}

\textbf{Results:}
\begin{itemize}[noitemsep]
    \item Fold 1 (B\&H: +90\%): 0/11 models beat buy-and-hold
    \item Fold 2 (B\&H: -4.3\%): \textbf{18/19 models beat buy-and-hold}
    \item Fold 3 (B\&H: +0.3\%): 2/19 models beat buy-and-hold
\end{itemize}

\subsubsection{Solana (SOL)}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{plots/section_12/12_sol_performance.png}
\caption{SOL Performance: Model P\&L vs. Buy-and-Hold across 3 folds}
\end{figure}

\textbf{Results:}
\begin{itemize}[noitemsep]
    \item Fold 1 (B\&H: +992\%): 0/11 models beat buy-and-hold
    \item Fold 2 (B\&H: +41\%): 0/19 models beat buy-and-hold
    \item Fold 3 (B\&H: -38\%): \textbf{19/19 models beat buy-and-hold}
\end{itemize}

\subsubsection{XRP}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{plots/section_12/12_xrp_performance.png}
\caption{XRP Performance: Model P\&L vs. Buy-and-Hold across 3 folds}
\end{figure}

\textbf{Results:}
\begin{itemize}[noitemsep]
    \item Fold 1 (B\&H: +21\%): 0/11 models beat buy-and-hold
    \item Fold 2 (B\&H: +421\%): 0/19 models beat buy-and-hold
    \item Fold 3 (B\&H: -28\%): \textbf{19/19 models beat buy-and-hold}
\end{itemize}

\subsubsection{Dogecoin (DOGE)}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{plots/section_12/12_doge_performance.png}
\caption{DOGE Performance: Model P\&L vs. Buy-and-Hold across 3 folds}
\end{figure}

\textbf{Results:}
\begin{itemize}[noitemsep]
    \item Fold 1 (B\&H: +215\%): 0/11 models beat buy-and-hold
    \item Fold 2 (B\&H: +76\%): 0/19 models beat buy-and-hold
    \item Fold 3 (B\&H: -53\%): \textbf{19/19 models beat buy-and-hold}
\end{itemize}

\subsection{Summary: Regime-Conditional Performance}

\begin{table}[H]
\centering
\caption{Models Beating Buy-and-Hold by Market Regime}
\begin{tabular}{lcccc}
\toprule
\textbf{Asset} & \textbf{Bull Fold} & \textbf{Mixed Fold} & \textbf{Bear Fold} & \textbf{Pattern} \\
\midrule
BTC & 0/11 & 0/19 & 17/19 & Bear protection \\
ETH & 0/11 & 18/19 & 2/19 & Bear protection \\
SOL & 0/11 & 0/19 & 19/19 & Bear protection \\
XRP & 0/11 & 0/19 & 19/19 & Bear protection \\
DOGE & 0/11 & 0/19 & 19/19 & Bear protection \\
\midrule
\textbf{Total} & \textbf{0/55} & \textbf{18/95} & \textbf{76/95} & --- \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Finding:} ML models beat buy-and-hold in \textbf{100\% of bear market scenarios} but only \textbf{$<$1\% of bull market scenarios}. Models provide defensive value, not alpha generation.

%==============================================================================
% 7. CONCLUSIONS
%==============================================================================
\section{Conclusion and Discussion}

\subsection{Summary of Findings}

This comprehensive evaluation of 19 machine learning models across 5 cryptocurrencies reveals several important insights for practitioners and researchers:

\subsubsection{Predictive Performance}
\begin{itemize}[noitemsep]
    \item All 19 models achieve accuracy significantly above random (50\%), confirming predictive value
    \item Best accuracy: Random Forest (52.57\%)
    \item Accuracy improvements over baseline are small but statistically significant (1-2\%)
\end{itemize}

\subsubsection{Economic Performance}
\begin{itemize}[noitemsep]
    \item Accuracy and P\&L show near-zero correlation (r = -0.014)
    \item Best P\&L: GRU+Combined\_Regime (+29.48 cumulative)
    \item Models optimizing for accuracy do not maximize profits
\end{itemize}

\subsubsection{Regime-Conditional Behavior}
\begin{itemize}[noitemsep]
    \item Models excel in bear markets: 100\% beat buy-and-hold
    \item Models fail in bull markets: $<$1\% beat buy-and-hold
    \item \textbf{Primary value is capital preservation, not alpha generation}
\end{itemize}

\subsubsection{Methodology Validation}
\begin{itemize}[noitemsep]
    \item K-fold CV inflates accuracy by 3.16\% due to temporal leakage
    \item Walk-forward validation with embargo is essential
    \item Cost-aware classification improves both accuracy and economic relevance
\end{itemize}

\subsection{Practical Recommendations}

\subsubsection{For Practitioners}

\begin{enumerate}
    \item \textbf{Model Selection:}
    \begin{itemize}[noitemsep]
        \item For accuracy stability: Random Forest
        \item For maximum P\&L: GRU+Combined\_Regime (with risk controls)
        \item For risk-adjusted returns: RF+Tech\_Regime (Sortino: 2.58)
    \end{itemize}

    \item \textbf{Regime-Conditional Strategy:}
    \begin{itemize}[noitemsep]
        \item Bull market (price $>$ 200-day MA): Use buy-and-hold
        \item Bear/uncertain market: Activate ML-based signals
        \item Expected Sharpe improvement: +0.3 to +0.5
    \end{itemize}

    \item \textbf{Risk Management:}
    \begin{itemize}[noitemsep]
        \item Apply isotonic calibration to Combined Regime models
        \item Implement 20\% maximum drawdown limits
        \item Monitor efficiency metrics weekly
    \end{itemize}
\end{enumerate}

\subsubsection{For Researchers}

\begin{enumerate}
    \item Always use walk-forward CV with embargo for financial time series
    \item Report economic metrics alongside predictive metrics
    \item Apply multiple testing corrections (Bonferroni) for model comparisons
    \item Evaluate regime-conditional performance, not just aggregate metrics
\end{enumerate}

\subsection{Limitations}

\begin{enumerate}
    \item \textbf{Data Period:} Results based on 2021-2025 data; may not generalize to future market cycles
    \item \textbf{Asset Selection:} Only major cryptocurrencies tested; small-cap tokens may behave differently
    \item \textbf{Execution Assumptions:} Perfect execution assumed; real-world slippage not fully modeled
    \item \textbf{Feature Set:} Limited to price and derivatives data; sentiment and on-chain data not included
    \item \textbf{Model Complexity:} Did not test Transformers or attention-based architectures
\end{enumerate}

\subsection{Future Work}

\begin{enumerate}
    \item \textbf{Adaptive Regime Detection:} Real-time regime switching with confidence thresholds
    \item \textbf{Reinforcement Learning:} Position sizing optimization via policy gradient methods
    \item \textbf{Alternative Data:} Integration of social sentiment, on-chain metrics, and order flow
    \item \textbf{Attention Mechanisms:} Transformer architectures for longer-range dependencies
    \item \textbf{Cross-Market Transfer:} Testing generalization to traditional equity and forex markets
\end{enumerate}

\subsection{Final Remarks}

This research provides definitive evidence that machine learning models for cryptocurrency trading are primarily \textbf{defensive instruments}. While models achieve statistically significant predictive accuracy above random, their economic value lies in capital preservation during market downturns rather than alpha generation during uptrends.

The disconnect between accuracy and profitability---where lower-accuracy models dramatically outperform higher-accuracy models in P\&L---challenges the conventional wisdom of maximizing predictive accuracy. Instead, practitioners should optimize for economic metrics directly and implement regime-conditional strategies that combine buy-and-hold for bull markets with ML-based protection during uncertainty.

\textbf{Our central thesis stands validated:} ML models serve as sophisticated risk management tools within a broader investment framework, capturing cryptocurrency upside through passive exposure while limiting drawdowns through ML-driven position management.

%==============================================================================
% REFERENCES
%==============================================================================
\newpage
\bibliographystyle{plainnat}
\begin{thebibliography}{20}

\bibitem[Alessandretti et al.(2018)]{alessandretti2018anticipating}
Alessandretti, L., ElBahrawy, A., Aiello, L.M., and Baronchelli, A. (2018).
\newblock Anticipating cryptocurrency prices using machine learning.
\newblock \emph{Complexity}, 2018.

\bibitem[Ang and Bekaert(2002)]{ang2002regime}
Ang, A. and Bekaert, G. (2002).
\newblock Regime switches in interest rates.
\newblock \emph{Journal of Business \& Economic Statistics}, 20(2):163--182.

\bibitem[Bailey et al.(2014)]{bailey2014probability}
Bailey, D.H., Borwein, J.M., L{\'o}pez de Prado, M., and Zhu, Q.J. (2014).
\newblock The probability of backtest overfitting.
\newblock \emph{Journal of Computational Finance}, 17(4):39--69.

\bibitem[Bao et al.(2017)]{bao2017deep}
Bao, W., Yue, J., and Rao, Y. (2017).
\newblock A deep learning framework for financial time series using stacked autoencoders and long-short term memory.
\newblock \emph{PloS one}, 12(7):e0180944.

\bibitem[Caporale et al.(2018)]{caporale2018modelling}
Caporale, G.M., Gil-Alana, L., and Plastun, A. (2018).
\newblock Modelling volatility in the cryptocurrency market.
\newblock \emph{CESifo Working Paper}.

\bibitem[Chen et al.(2020)]{chen2020bitcoin}
Chen, W., Xu, H., Jia, L., and Gao, Y. (2020).
\newblock Machine learning model for bitcoin exchange rate prediction using economic and technology determinants.
\newblock \emph{International Journal of Forecasting}.

\bibitem[de Prado(2018)]{de2018advances}
de Prado, M.L. (2018).
\newblock \emph{Advances in Financial Machine Learning}.
\newblock John Wiley \& Sons.

\bibitem[Fama(1970)]{fama1970efficient}
Fama, E.F. (1970).
\newblock Efficient capital markets: A review of theory and empirical work.
\newblock \emph{The Journal of Finance}, 25(2):383--417.

\bibitem[Fischer and Krauss(2018)]{fischer2018deep}
Fischer, T. and Krauss, C. (2018).
\newblock Deep learning with long short-term memory networks for financial market predictions.
\newblock \emph{European Journal of Operational Research}, 270(2):654--669.

\bibitem[Guo et al.(2017)]{guo2017calibration}
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K.Q. (2017).
\newblock On calibration of modern neural networks.
\newblock \emph{International Conference on Machine Learning}, pages 1321--1330.

\bibitem[Hamilton(1989)]{hamilton1989new}
Hamilton, J.D. (1989).
\newblock A new approach to the economic analysis of nonstationary time series and the business cycle.
\newblock \emph{Econometrica}, pages 357--384.

\bibitem[Jiang et al.(2017)]{jiang2017cryptocurrency}
Jiang, Z., Xu, D., and Liang, J. (2017).
\newblock A deep reinforcement learning framework for the financial portfolio management problem.
\newblock \emph{arXiv preprint arXiv:1706.10059}.

\bibitem[McNally et al.(2018)]{mcnally2018predicting}
McNally, S., Roche, J., and Caton, S. (2018).
\newblock Predicting the price of bitcoin using machine learning.
\newblock \emph{26th Euromicro International Conference on Parallel, Distributed and Network-based Processing}.

\bibitem[Platt(1999)]{platt1999probabilistic}
Platt, J. (1999).
\newblock Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.
\newblock \emph{Advances in Large Margin Classifiers}, 10(3):61--74.

\bibitem[Zadrozny and Elkan(2002)]{zadrozny2002transforming}
Zadrozny, B. and Elkan, C. (2002).
\newblock Transforming classifier scores into accurate multiclass probability estimates.
\newblock \emph{Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}.

\end{thebibliography}

\end{document}
