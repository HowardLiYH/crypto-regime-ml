{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0adf9881",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Random Forest Multi-Horizon Trading Signal Generator\n",
    "Complete implementation for cryptocurrency trading with Random Forest classification/regression\n",
    "\n",
    "Key features:\n",
    "- One Random Forest model per horizon (1, 3, 6 bars)\n",
    "- Native probability estimates (no calibration needed for RF)\n",
    "- Walk-forward cross-validation\n",
    "- Isotonic + conformal calibration\n",
    "- JSON feed output for trading agent\n",
    "- Feature importance analysis\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaf9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# CONFIGURATION\n",
    "# =========================================\n",
    "DATA_DIR = Path(\"/Users/nitinlodha/Desktop/ML/ML_Project/Bybit_CSV_Data\")\n",
    "FILES = {\n",
    "    \"BTC\": DATA_DIR / \"Bybit_BTC.csv\",\n",
    "    \"ETH\": DATA_DIR / \"Bybit_ETH.csv\",\n",
    "    \"SOL\": DATA_DIR / \"Bybit_SOL.csv\",\n",
    "    \"XRP\": DATA_DIR / \"Bybit_XRP.csv\",\n",
    "    \"DOGE\": DATA_DIR / \"Bybit_DOGE.csv\",\n",
    "}\n",
    "\n",
    "HORIZONS = [1, 3, 6]  # Forecast horizons in 4-hour bars\n",
    "DEFAULT_COST_BP = {1: 8.0, 3: 10.0, 6: 12.0}  # Trading costs in basis points\n",
    "\n",
    "# Policy thresholds\n",
    "TAU_P = 0.60        # Probability gate for P(edge > cost)\n",
    "TAU_MU = 0.0005     # Expected-return gate (log-return)\n",
    "LAM = 2.0           # Kelly-lite multiplier\n",
    "W_MAX = 0.50        # Max gross position (50% notional)\n",
    "\n",
    "MODEL_VERSION = \"rf_multiH_v1.0\"\n",
    "CALIBRATION_VERSION = \"iso+conformal_v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45123d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =========================================\n",
    "def bp_to_logret(bp: float) -> float:\n",
    "    \"\"\"Convert basis points to log-return units.\"\"\"\n",
    "    return bp * 1e-4\n",
    "\n",
    "\n",
    "def _find_close_column(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Find the close price column in a dataframe.\"\"\"\n",
    "    lower = {c.lower(): c for c in df.columns}\n",
    "    for key in (\"close\", \"closing_price\", \"close_price\", \"price_close\", \"last\", \"c\"):\n",
    "        if key in lower:\n",
    "            return lower[key]\n",
    "    # Fallback: any single float column\n",
    "    float_cols = [c for c in df.columns if pd.api.types.is_float_dtype(df[c])]\n",
    "    if len(float_cols) == 1:\n",
    "        return float_cols[0]\n",
    "    raise ValueError(\"Cannot identify 'close' column.\")\n",
    "\n",
    "\n",
    "def cumulative_log_returns(price: pd.Series, h: int) -> pd.Series:\n",
    "    \"\"\"Compute log(P_{t+h}/P_t) aligned to t.\"\"\"\n",
    "    return np.log(price.shift(-h) / price).dropna()\n",
    "\n",
    "\n",
    "def brier_score(y: np.ndarray, p: np.ndarray) -> float:\n",
    "    \"\"\"Brier score for probability calibration.\"\"\"\n",
    "    return float(np.mean((y - p) ** 2))\n",
    "\n",
    "\n",
    "def expected_calibration_error(y: np.ndarray, p: np.ndarray, bins: int = 10) -> float:\n",
    "    \"\"\"Expected Calibration Error (ECE).\"\"\"\n",
    "    edges = np.linspace(0, 1, bins + 1)\n",
    "    ece = 0.0\n",
    "    for i in range(bins):\n",
    "        m = (p >= edges[i]) & (p < edges[i+1])\n",
    "        if m.sum() == 0:\n",
    "            continue\n",
    "        ece += (m.sum()/len(p)) * np.abs(np.mean(y[m]) - np.mean(p[m]))\n",
    "    return float(ece)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4df6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# FEATURE ENGINEERING\n",
    "# =========================================\n",
    "def make_feature_table(close: pd.Series):\n",
    "    \"\"\"\n",
    "    Build feature table from close prices.\n",
    "\n",
    "    Features:\n",
    "    - Returns at multiple lags (1, 3, 6 bars)\n",
    "    - Rolling volatility (6, 12 bars)\n",
    "    - Moving average ratio (log MA10/MA20)\n",
    "\n",
    "    Returns:\n",
    "        df: DataFrame with price and features\n",
    "        X: Feature matrix (numpy array)\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(index=close.index)\n",
    "    df[\"price\"] = close.astype(float)\n",
    "\n",
    "    # Log returns\n",
    "    df[\"ret_1\"] = np.log(df[\"price\"] / df[\"price\"].shift(1))\n",
    "    df[\"ret_3\"] = np.log(df[\"price\"] / df[\"price\"].shift(3))\n",
    "    df[\"ret_6\"] = np.log(df[\"price\"] / df[\"price\"].shift(6))\n",
    "\n",
    "    # Volatility\n",
    "    df[\"vol_6\"] = df[\"ret_1\"].rolling(6).std()\n",
    "    df[\"vol_12\"] = df[\"ret_1\"].rolling(12).std()\n",
    "\n",
    "    # Moving average ratio\n",
    "    ma_10 = df[\"price\"].rolling(10).mean()\n",
    "    ma_20 = df[\"price\"].rolling(20).mean()\n",
    "    df[\"ma_ratio\"] = np.log(ma_10 / ma_20)\n",
    "\n",
    "    # Drop NaN rows\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Feature matrix (exclude price)\n",
    "    feat_cols = [c for c in df.columns if c != \"price\"]\n",
    "    X = df[feat_cols].values.astype(float)\n",
    "\n",
    "    return df, X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c85e3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# WALK-FORWARD CV\n",
    "# =========================================\n",
    "def purged_walkforward_slices(n: int, n_folds: int = 3, embargo: int = 24):\n",
    "    \"\"\"\n",
    "    Generate (train, val, test) slices for walk-forward CV with embargo.\n",
    "\n",
    "    Args:\n",
    "        n: Total number of samples\n",
    "        n_folds: Number of folds\n",
    "        embargo: Gap between train/val and val/test (in bars)\n",
    "\n",
    "    Returns:\n",
    "        List of ((train_start, train_end), (val_start, val_end), (test_start, test_end))\n",
    "    \"\"\"\n",
    "    fold_size = n // (n_folds + 2)\n",
    "    slices = []\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        train_end = (i + 1) * fold_size\n",
    "        val_start = train_end + embargo\n",
    "        val_end = val_start + fold_size\n",
    "        test_start = val_end + embargo\n",
    "        test_end = min(test_start + fold_size, n)\n",
    "\n",
    "        if test_end - test_start < fold_size // 2:\n",
    "            break\n",
    "\n",
    "        slices.append((\n",
    "            (0, train_end),\n",
    "            (val_start, val_end),\n",
    "            (test_start, test_end)\n",
    "        ))\n",
    "\n",
    "    return slices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec15200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# RANDOM FOREST MODEL\n",
    "# =========================================\n",
    "@dataclass\n",
    "class RFSnapshot:\n",
    "    \"\"\"Container for trained Random Forest model and preprocessing.\"\"\"\n",
    "    clf: RandomForestClassifier | RandomForestRegressor\n",
    "    scaler: StandardScaler\n",
    "    model_type: str  # 'classification' or 'regression'\n",
    "    horizon: int\n",
    "    feature_names: list[str]\n",
    "    feature_importances: np.ndarray | None = None\n",
    "\n",
    "\n",
    "def fit_rf_classifier(X_train: np.ndarray, y_train: np.ndarray,\n",
    "                      horizon: int,\n",
    "                      feature_names: list[str] = None,\n",
    "                      random_state: int = 123,\n",
    "                      n_estimators: int = 200,\n",
    "                      max_depth: int | None = 10,\n",
    "                      min_samples_split: int = 20,\n",
    "                      min_samples_leaf: int = 10) -> RFSnapshot:\n",
    "    \"\"\"\n",
    "    Train Random Forest classifier for binary edge detection.\n",
    "\n",
    "    Args:\n",
    "        X_train: Feature matrix (T, D)\n",
    "        y_train: Binary labels (1 if return > cost, else 0)\n",
    "        horizon: Forecast horizon in bars\n",
    "        feature_names: List of feature names for importance analysis\n",
    "        n_estimators: Number of trees\n",
    "        max_depth: Maximum tree depth (None = unlimited)\n",
    "        min_samples_split: Minimum samples to split node\n",
    "        min_samples_leaf: Minimum samples in leaf\n",
    "\n",
    "    Returns:\n",
    "        RFSnapshot with trained classifier\n",
    "    \"\"\"\n",
    "    # Standardize features (optional for RF, but helps with interpretability)\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_scaled = scaler.transform(X_train)\n",
    "\n",
    "    # Random Forest Classifier\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        class_weight='balanced',  # Handle imbalanced classes\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,  # Use all CPU cores\n",
    "        bootstrap=True,\n",
    "        oob_score=True,  # Out-of-bag score for validation\n",
    "        max_features='sqrt'  # Number of features per split\n",
    "    )\n",
    "\n",
    "    clf.fit(X_scaled, y_train)\n",
    "\n",
    "    # Feature importances\n",
    "    feature_importances = clf.feature_importances_ if hasattr(clf, 'feature_importances_') else None\n",
    "\n",
    "    return RFSnapshot(\n",
    "        clf=clf,\n",
    "        scaler=scaler,\n",
    "        model_type='classification',\n",
    "        horizon=horizon,\n",
    "        feature_names=feature_names or [f\"feat_{i}\" for i in range(X_train.shape[1])],\n",
    "        feature_importances=feature_importances\n",
    "    )\n",
    "\n",
    "\n",
    "def fit_rf_regressor(X_train: np.ndarray, y_train: np.ndarray,\n",
    "                     horizon: int,\n",
    "                     feature_names: list[str] = None,\n",
    "                     random_state: int = 123,\n",
    "                     n_estimators: int = 200,\n",
    "                     max_depth: int | None = 10,\n",
    "                     min_samples_split: int = 20,\n",
    "                     min_samples_leaf: int = 10) -> RFSnapshot:\n",
    "    \"\"\"\n",
    "    Train Random Forest regressor to directly predict continuous returns.\n",
    "\n",
    "    Args:\n",
    "        X_train: Feature matrix\n",
    "        y_train: Continuous log returns\n",
    "\n",
    "    Returns:\n",
    "        RFSnapshot with trained regressor\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_scaled = scaler.transform(X_train)\n",
    "\n",
    "    reg = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        bootstrap=True,\n",
    "        oob_score=True,\n",
    "        max_features='sqrt'\n",
    "    )\n",
    "\n",
    "    reg.fit(X_scaled, y_train)\n",
    "\n",
    "    feature_importances = reg.feature_importances_ if hasattr(reg, 'feature_importances_') else None\n",
    "\n",
    "    return RFSnapshot(\n",
    "        clf=reg,\n",
    "        scaler=scaler,\n",
    "        model_type='regression',\n",
    "        horizon=horizon,\n",
    "        feature_names=feature_names or [f\"feat_{i}\" for i in range(X_train.shape[1])],\n",
    "        feature_importances=feature_importances\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4b3ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# FORECASTING\n",
    "# =========================================\n",
    "def forecast_multi_horizon_rf(\n",
    "    snapshots: dict[int, RFSnapshot],\n",
    "    X_seg: np.ndarray,\n",
    "    price_seg: pd.Series,\n",
    "    horizons: list[int],\n",
    "    cost_bp: dict[int, float] | None = None,\n",
    "    compute_tree_variance: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate multi-horizon forecasts using trained Random Forests.\n",
    "\n",
    "    Random Forests provide natural uncertainty estimates via tree variance,\n",
    "    which is more principled than bootstrap for ensemble methods.\n",
    "\n",
    "    Args:\n",
    "        snapshots: Dict mapping horizon -> trained RFSnapshot\n",
    "        X_seg: Feature matrix for forecast segment\n",
    "        price_seg: Corresponding price series\n",
    "        horizons: List of forecast horizons\n",
    "        cost_bp: Trading costs in basis points\n",
    "        compute_tree_variance: If True, compute prediction variance from trees\n",
    "\n",
    "    Returns:\n",
    "        out: dict[horizon] -> DataFrame with predictions\n",
    "        cost_log: dict[horizon] -> cost in log-return units\n",
    "    \"\"\"\n",
    "    if cost_bp is None:\n",
    "        cost_bp = {h: DEFAULT_COST_BP.get(h, 8.0) for h in horizons}\n",
    "    cost_log = {h: bp_to_logret(float(cost_bp[h])) for h in horizons}\n",
    "\n",
    "    Tseg = X_seg.shape[0]\n",
    "    idx = price_seg.index\n",
    "    out = {}\n",
    "\n",
    "    for h in horizons:\n",
    "        if h not in snapshots:\n",
    "            print(f\"Warning: No model for horizon {h}, skipping\")\n",
    "            continue\n",
    "\n",
    "        snap = snapshots[h]\n",
    "        out_h = pd.DataFrame(index=idx[:-h] if h < Tseg else idx[:0])\n",
    "        T_h = Tseg - h\n",
    "\n",
    "        if T_h <= 0:\n",
    "            out[h] = out_h\n",
    "            continue\n",
    "\n",
    "        # Scale features\n",
    "        X_scaled = snap.scaler.transform(X_seg[:T_h])\n",
    "\n",
    "        if snap.model_type == 'classification':\n",
    "            # Get probabilities (RF provides well-calibrated probabilities)\n",
    "            p_edge = snap.clf.predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "            # Expected return estimation (probability-weighted)\n",
    "            mu = p_edge * (cost_log[h] + 0.002) + (1 - p_edge) * (-cost_log[h] - 0.001)\n",
    "\n",
    "            # Uncertainty from tree variance\n",
    "            if compute_tree_variance:\n",
    "                std_h, q10, q50, q90 = _tree_variance_classification(\n",
    "                    snap, X_scaled, cost_log[h]\n",
    "                )\n",
    "            else:\n",
    "                # Simple uncertainty estimate\n",
    "                std_h = np.sqrt(p_edge * (1 - p_edge)) * 0.03  # Binomial variance scaled\n",
    "                q10 = mu - 1.28 * std_h  # ~10th percentile\n",
    "                q50 = mu\n",
    "                q90 = mu + 1.28 * std_h  # ~90th percentile\n",
    "\n",
    "        elif snap.model_type == 'regression':\n",
    "            # Direct return prediction\n",
    "            mu = snap.clf.predict(X_scaled)\n",
    "\n",
    "            # Probability via sigmoid transform\n",
    "            p_edge = 1.0 / (1.0 + np.exp(-10 * (mu - cost_log[h])))\n",
    "\n",
    "            if compute_tree_variance:\n",
    "                std_h, q10, q50, q90 = _tree_variance_regression(snap, X_scaled)\n",
    "            else:\n",
    "                # Simple uncertainty estimate\n",
    "                std_h = np.full(T_h, 0.015)\n",
    "                q10 = mu - 0.025\n",
    "                q50 = mu\n",
    "                q90 = mu + 0.025\n",
    "\n",
    "        # Populate DataFrame\n",
    "        p_now = price_seg.iloc[:T_h].values\n",
    "\n",
    "        out_h['mu'] = mu\n",
    "        out_h['std'] = std_h\n",
    "        out_h['p_edge_raw'] = p_edge\n",
    "        out_h['ret_q10'] = q10\n",
    "        out_h['ret_q50'] = q50\n",
    "        out_h['ret_q90'] = q90\n",
    "        out_h['price_pred'] = p_now * np.exp(mu)\n",
    "        out_h['price_q10'] = p_now * np.exp(q10)\n",
    "        out_h['price_q50'] = p_now * np.exp(q50)\n",
    "        out_h['price_q90'] = p_now * np.exp(q90)\n",
    "\n",
    "        out[h] = out_h\n",
    "\n",
    "    return out, cost_log\n",
    "\n",
    "\n",
    "def _tree_variance_classification(snap, X_scaled, cost_threshold):\n",
    "    \"\"\"\n",
    "    Compute uncertainty from individual tree predictions (classification).\n",
    "\n",
    "    Each tree in the forest votes for a class. We can use the distribution\n",
    "    of these votes to estimate uncertainty.\n",
    "    \"\"\"\n",
    "    # Get predictions from all trees\n",
    "    tree_probs = np.array([tree.predict_proba(X_scaled)[:, 1]\n",
    "                           for tree in snap.clf.estimators_])  # (n_trees, T)\n",
    "\n",
    "    # Variance across trees\n",
    "    std = np.std(tree_probs, axis=0)\n",
    "\n",
    "    # Convert probabilities to return estimates\n",
    "    mu_samples = tree_probs * (cost_threshold + 0.002) + (1 - tree_probs) * (-cost_threshold - 0.001)\n",
    "\n",
    "    q10 = np.percentile(mu_samples, 10, axis=0)\n",
    "    q50 = np.percentile(mu_samples, 50, axis=0)\n",
    "    q90 = np.percentile(mu_samples, 90, axis=0)\n",
    "\n",
    "    return std, q10, q50, q90\n",
    "\n",
    "\n",
    "def _tree_variance_regression(snap, X_scaled):\n",
    "    \"\"\"\n",
    "    Compute uncertainty from individual tree predictions (regression).\n",
    "    \"\"\"\n",
    "    # Get predictions from all trees\n",
    "    tree_preds = np.array([tree.predict(X_scaled)\n",
    "                          for tree in snap.clf.estimators_])  # (n_trees, T)\n",
    "\n",
    "    # Statistics across trees\n",
    "    std = np.std(tree_preds, axis=0)\n",
    "    q10 = np.percentile(tree_preds, 10, axis=0)\n",
    "    q50 = np.percentile(tree_preds, 50, axis=0)\n",
    "    q90 = np.percentile(tree_preds, 90, axis=0)\n",
    "\n",
    "    return std, q10, q50, q90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef3d09d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# CALIBRATION\n",
    "# =========================================\n",
    "@dataclass\n",
    "class ProbCalibrator:\n",
    "    \"\"\"Probability calibrator using isotonic regression.\"\"\"\n",
    "    method: str\n",
    "    iso: IsotonicRegression | None = None\n",
    "\n",
    "\n",
    "def fit_prob_calibrator_isotonic(p_raw: np.ndarray, y: np.ndarray,\n",
    "                                 min_points: int = 30) -> ProbCalibrator:\n",
    "    \"\"\"Fit isotonic regression p_raw -> y.\"\"\"\n",
    "    p_raw = np.asarray(p_raw, float)\n",
    "    y = np.asarray(y, float)\n",
    "    m = np.isfinite(p_raw) & np.isfinite(y)\n",
    "    p, t = p_raw[m], y[m]\n",
    "    if p.size < min_points or np.unique(p).size < 3:\n",
    "        return ProbCalibrator(method=\"identity\", iso=None)\n",
    "    iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "    iso.fit(p, t)\n",
    "    return ProbCalibrator(method=\"isotonic\", iso=iso)\n",
    "\n",
    "\n",
    "def apply_prob_calibrator(cal: ProbCalibrator, p_raw: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Apply probability calibrator.\"\"\"\n",
    "    p_raw = np.asarray(p_raw, float)\n",
    "    if cal.method == \"isotonic\":\n",
    "        return cal.iso.predict(p_raw)\n",
    "    return p_raw\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class IntervalCalibrator:\n",
    "    \"\"\"Conformal prediction interval calibrator.\"\"\"\n",
    "    method: str\n",
    "    q_alpha: float\n",
    "    alpha: float\n",
    "\n",
    "\n",
    "def fit_conformal_interval(residuals: np.ndarray, alpha: float = 0.2) -> IntervalCalibrator:\n",
    "    \"\"\"Fit conformal prediction intervals.\"\"\"\n",
    "    resid = np.asarray(residuals, float)\n",
    "    resid = resid[np.isfinite(resid)]\n",
    "    q = float(np.quantile(np.abs(resid), 1 - alpha)) if resid.size > 0 else 0.0\n",
    "    return IntervalCalibrator(method=\"conformal_abs\", q_alpha=q, alpha=alpha)\n",
    "\n",
    "\n",
    "def apply_conformal_interval(cal: IntervalCalibrator, mu: np.ndarray):\n",
    "    \"\"\"Apply conformal prediction intervals.\"\"\"\n",
    "    mu = np.asarray(mu, float)\n",
    "    return mu - cal.q_alpha, mu + cal.q_alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16be3011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# FEATURE IMPORTANCE ANALYSIS\n",
    "# =========================================\n",
    "def analyze_feature_importance(snapshots: dict[int, RFSnapshot]):\n",
    "    \"\"\"\n",
    "    Analyze and print feature importances across horizons.\n",
    "\n",
    "    Random Forests provide natural feature importance scores based on\n",
    "    how much each feature reduces impurity across all trees.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for h, snap in snapshots.items():\n",
    "        if snap.feature_importances is None:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nHorizon {h}:\")\n",
    "\n",
    "        # Sort features by importance\n",
    "        indices = np.argsort(snap.feature_importances)[::-1]\n",
    "\n",
    "        for i, idx in enumerate(indices[:10]):  # Top 10 features\n",
    "            print(f\"  {i+1}. {snap.feature_names[idx]}: {snap.feature_importances[idx]:.4f}\")\n",
    "\n",
    "        # OOB score if available\n",
    "        if hasattr(snap.clf, 'oob_score_'):\n",
    "            print(f\"  Out-of-bag score: {snap.clf.oob_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc22de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# MAIN TRAINING PIPELINE\n",
    "# =========================================\n",
    "def run_rf_for_symbol(symbol: str, path: Path,\n",
    "                      horizons: list[int] = HORIZONS,\n",
    "                      n_folds: int = 3,\n",
    "                      embargo: int = 24,\n",
    "                      model_type: str = 'classification',\n",
    "                      n_estimators: int = 200,\n",
    "                      max_depth: int | None = 10,\n",
    "                      compute_tree_variance: bool = True):\n",
    "    \"\"\"\n",
    "    Train and evaluate Random Forest models for one symbol.\n",
    "\n",
    "    Args:\n",
    "        symbol: Asset symbol\n",
    "        path: Path to CSV file\n",
    "        horizons: Forecast horizons in bars\n",
    "        n_folds: Number of walk-forward folds\n",
    "        embargo: Embargo period between folds\n",
    "        model_type: 'classification' or 'regression'\n",
    "        n_estimators: Number of trees in forest\n",
    "        max_depth: Maximum tree depth\n",
    "        compute_tree_variance: Use tree variance for uncertainty\n",
    "\n",
    "    Returns:\n",
    "        results: dict[horizon] -> dict with 'val', 'test', 'diag' DataFrames\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df_raw = pd.read_csv(path)\n",
    "    close_col = _find_close_column(df_raw)\n",
    "    close = pd.Series(df_raw[close_col].astype(float).values,\n",
    "                      index=pd.RangeIndex(len(df_raw)), name=\"close\")\n",
    "\n",
    "    feat_df, X = make_feature_table(close)\n",
    "    price = feat_df[\"price\"]\n",
    "    n = len(price)\n",
    "\n",
    "    # Get feature names\n",
    "    feature_names = [c for c in feat_df.columns if c != \"price\"]\n",
    "\n",
    "    folds = purged_walkforward_slices(n, n_folds=n_folds, embargo=embargo)\n",
    "\n",
    "    results = {h: {\"val\": [], \"test\": [], \"diag\": []} for h in horizons}\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Random Forest for {symbol}\")\n",
    "    print(f\"Model type: {model_type}\")\n",
    "    print(f\"Horizons: {horizons}\")\n",
    "    print(f\"Folds: {n_folds}\")\n",
    "    print(f\"Trees: {n_estimators}\")\n",
    "    print(f\"Max depth: {max_depth}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    for fold_idx, ((s0,e0), (s1,e1), (s2,e2)) in enumerate(folds):\n",
    "        print(f\"Fold {fold_idx + 1}/{len(folds)}: Train[{s0}:{e0}] Val[{s1}:{e1}] Test[{s2}:{e2}]\")\n",
    "\n",
    "        # Train one RF per horizon\n",
    "        snapshots = {}\n",
    "\n",
    "        for h in horizons:\n",
    "            print(f\"  Training h={h}...\", end=\" \")\n",
    "\n",
    "            # Create labels for this horizon\n",
    "            ret_train = cumulative_log_returns(price.iloc[s0:e0], h)\n",
    "\n",
    "            # Align features and labels\n",
    "            n_train = min(len(X[s0:e0]), len(ret_train))\n",
    "            X_train_aligned = X[s0:s0+n_train]\n",
    "            ret_train_aligned = ret_train.iloc[:n_train]\n",
    "\n",
    "            if len(X_train_aligned) < 50:\n",
    "                print(\"SKIP (insufficient data)\")\n",
    "                continue\n",
    "\n",
    "            if model_type == 'classification':\n",
    "                # Binary classification\n",
    "                y_train = (ret_train_aligned.values > bp_to_logret(DEFAULT_COST_BP[h])).astype(int)\n",
    "\n",
    "                # Check class balance\n",
    "                pos_frac = y_train.mean()\n",
    "                if pos_frac < 0.1 or pos_frac > 0.9:\n",
    "                    print(f\"WARN (imbalanced: {pos_frac:.2%} positive)\", end=\" \")\n",
    "\n",
    "                snap = fit_rf_classifier(\n",
    "                    X_train_aligned, y_train,\n",
    "                    horizon=h,\n",
    "                    feature_names=feature_names,\n",
    "                    random_state=123 + fold_idx,\n",
    "                    n_estimators=n_estimators,\n",
    "                    max_depth=max_depth\n",
    "                )\n",
    "            else:\n",
    "                # Regression\n",
    "                y_train = ret_train_aligned.values\n",
    "                snap = fit_rf_regressor(\n",
    "                    X_train_aligned, y_train,\n",
    "                    horizon=h,\n",
    "                    feature_names=feature_names,\n",
    "                    random_state=123 + fold_idx,\n",
    "                    n_estimators=n_estimators,\n",
    "                    max_depth=max_depth\n",
    "                )\n",
    "\n",
    "            snapshots[h] = snap\n",
    "\n",
    "            # Print OOB score\n",
    "            if hasattr(snap.clf, 'oob_score_'):\n",
    "                print(f\"✓ (OOB: {snap.clf.oob_score_:.3f})\")\n",
    "            else:\n",
    "                print(\"✓\")\n",
    "\n",
    "        if not snapshots:\n",
    "            print(\"  No models trained, skipping fold\")\n",
    "            continue\n",
    "\n",
    "        # Analyze feature importance (first fold only)\n",
    "        if fold_idx == 0:\n",
    "            analyze_feature_importance(snapshots)\n",
    "\n",
    "        # Forecast on validation and test\n",
    "        print(\"  Forecasting validation...\", end=\" \")\n",
    "        out_val_raw, cost_log = forecast_multi_horizon_rf(\n",
    "            snapshots=snapshots,\n",
    "            X_seg=X[s1:e1],\n",
    "            price_seg=price.iloc[s1:e1],\n",
    "            horizons=horizons,\n",
    "            compute_tree_variance=compute_tree_variance\n",
    "        )\n",
    "        print(\"✓\")\n",
    "\n",
    "        print(\"  Forecasting test...\", end=\" \")\n",
    "        out_test_raw, _ = forecast_multi_horizon_rf(\n",
    "            snapshots=snapshots,\n",
    "            X_seg=X[s2:e2],\n",
    "            price_seg=price.iloc[s2:e2],\n",
    "            horizons=horizons,\n",
    "            compute_tree_variance=compute_tree_variance\n",
    "        )\n",
    "        print(\"✓\")\n",
    "\n",
    "        # Calibration (same as SVM/HMM)\n",
    "        for h in horizons:\n",
    "            if h not in out_val_raw or h not in out_test_raw:\n",
    "                continue\n",
    "\n",
    "            ret_val = cumulative_log_returns(price.iloc[s1:e1], h)\n",
    "            idx_common = out_val_raw[h].index.intersection(ret_val.index)\n",
    "\n",
    "            if len(idx_common) == 0:\n",
    "                continue\n",
    "\n",
    "            dfV = out_val_raw[h].loc[idx_common].copy()\n",
    "            maskV = np.isfinite(dfV[\"p_edge_raw\"].values) & np.isfinite(dfV[\"mu\"].values)\n",
    "            dfV = dfV[maskV]\n",
    "\n",
    "            if len(dfV) < 20:\n",
    "                continue\n",
    "\n",
    "            ret_val_aligned = ret_val.loc[dfV.index]\n",
    "            y_val = (ret_val_aligned.values > cost_log[h]).astype(int)\n",
    "            p_raw_val = dfV[\"p_edge_raw\"].values\n",
    "            mu_val = dfV[\"mu\"].values\n",
    "\n",
    "            # Fit calibrators\n",
    "            cal_prob = fit_prob_calibrator_isotonic(p_raw_val, y_val, min_points=20)\n",
    "            resid_val = ret_val_aligned.values - mu_val\n",
    "            cal_pi = fit_conformal_interval(resid_val, alpha=0.2)\n",
    "\n",
    "            # Apply to test\n",
    "            ret_test = cumulative_log_returns(price.iloc[s2:e2], h)\n",
    "            idx_test_common = out_test_raw[h].index.intersection(ret_test.index)\n",
    "            dfT = out_test_raw[h].loc[idx_test_common].copy()\n",
    "\n",
    "            maskT = np.isfinite(dfT[\"p_edge_raw\"].values) & np.isfinite(dfT[\"mu\"].values)\n",
    "            dfT = dfT[maskT]\n",
    "\n",
    "            if len(dfT) == 0:\n",
    "                continue\n",
    "\n",
    "            dfT[\"p_edge\"] = apply_prob_calibrator(cal_prob, dfT[\"p_edge_raw\"].values)\n",
    "            mu_test = dfT[\"mu\"].values\n",
    "            lo, hi = apply_conformal_interval(cal_pi, mu_test)\n",
    "            dfT[\"ret_lo\"] = lo\n",
    "            dfT[\"ret_hi\"] = hi\n",
    "\n",
    "            p_now = price.loc[dfT.index].values\n",
    "            dfT[\"price_lo\"] = p_now * np.exp(lo)\n",
    "            dfT[\"price_hi\"] = p_now * np.exp(hi)\n",
    "\n",
    "            dfT[\"edge\"] = dfT[\"mu\"] - cost_log[h]\n",
    "            dfT[\"risk_edge\"] = (dfT[\"mu\"] - cost_log[h]) / (dfT[\"std\"] + 1e-12)\n",
    "\n",
    "            results[h][\"test\"].append(dfT)\n",
    "\n",
    "            # Diagnostics\n",
    "            if cal_prob.method == \"isotonic\":\n",
    "                p_cal_val = apply_prob_calibrator(cal_prob, p_raw_val)\n",
    "                brier = brier_score(y_val, p_cal_val)\n",
    "                ece = expected_calibration_error(y_val, p_cal_val)\n",
    "            else:\n",
    "                brier = brier_score(y_val, p_raw_val)\n",
    "                ece = expected_calibration_error(y_val, p_raw_val)\n",
    "\n",
    "            coverage = float(np.mean((resid_val >= -cal_pi.q_alpha) & (resid_val <= cal_pi.q_alpha)))\n",
    "\n",
    "            diag = {\n",
    "                \"h\": h,\n",
    "                \"brier_val\": float(brier),\n",
    "                \"ece_val\": float(ece),\n",
    "                \"pi_coverage_val\": coverage\n",
    "            }\n",
    "            results[h][\"diag\"].append(diag)\n",
    "\n",
    "            # Store validation\n",
    "            dfV[\"p_edge\"] = apply_prob_calibrator(cal_prob, dfV[\"p_edge_raw\"].values)\n",
    "            loV, hiV = apply_conformal_interval(cal_pi, mu_val)\n",
    "            dfV[\"ret_lo\"], dfV[\"ret_hi\"] = loV, hiV\n",
    "            results[h][\"val\"].append(dfV)\n",
    "\n",
    "    # Concatenate folds\n",
    "    for h in horizons:\n",
    "        for split in (\"val\", \"test\"):\n",
    "            if results[h][split]:\n",
    "                results[h][split] = pd.concat(results[h][split]).sort_index()\n",
    "            else:\n",
    "                results[h][split] = pd.DataFrame()\n",
    "\n",
    "    print(f\"\\nCompleted {symbol}\\n\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f74d1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# JSON EXPORT\n",
    "# =========================================\n",
    "def build_json_records(all_outputs: dict,\n",
    "                       model_version: str = MODEL_VERSION,\n",
    "                       calibration_version: str = CALIBRATION_VERSION,\n",
    "                       horizons: list[int] = HORIZONS):\n",
    "    \"\"\"Build JSONL records for trading agent.\"\"\"\n",
    "    records = []\n",
    "    for sym, res in all_outputs.items():\n",
    "        for h in horizons:\n",
    "            df = res[h][\"test\"]\n",
    "            if isinstance(df, list) or isinstance(df, tuple):\n",
    "                df = pd.concat(df).sort_index()\n",
    "            for t, row in df.iterrows():\n",
    "                rec = {\n",
    "                    \"timestamp_index\": int(t),\n",
    "                    \"symbol\": sym,\n",
    "                    \"horizon_bars\": int(h),\n",
    "                    \"model_version\": model_version,\n",
    "                    \"calibration_version\": calibration_version,\n",
    "                    \"signals\": {\n",
    "                        \"expected_return\": float(row[\"mu\"]),\n",
    "                        \"stdev_return\": float(row[\"std\"]),\n",
    "                        \"p_edge_gt_cost\": float(row[\"p_edge\"]),\n",
    "                        \"predicted_price\": float(row[\"price_pred\"]),\n",
    "                        \"price_PI\": {\n",
    "                            \"p10\": float(row[\"price_q10\"]),\n",
    "                            \"p50\": float(row[\"price_q50\"]),\n",
    "                            \"p90\": float(row[\"price_q90\"])\n",
    "                        }\n",
    "                    },\n",
    "                    \"policy_suggestions\": {\n",
    "                        \"gate_threshold_p\": TAU_P,\n",
    "                        \"gate_threshold_edge\": TAU_MU,\n",
    "                        \"suggested_action\": \"buy\" if (row[\"p_edge\"]>=TAU_P and row[\"edge\"]>=TAU_MU and row[\"mu\"]>0)\n",
    "                                            else (\"sell\" if (row[\"p_edge\"]>=TAU_P and row[\"edge\"]>=TAU_MU and row[\"mu\"]<0)\n",
    "                                                  else \"hold\")\n",
    "                    }\n",
    "                }\n",
    "                records.append(rec)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22535a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Random Forest for BTC\n",
      "Model type: classification\n",
      "Horizons: [1, 3, 6]\n",
      "Folds: 3\n",
      "Trees: 200\n",
      "Max depth: 10\n",
      "============================================================\n",
      "\n",
      "Fold 1/3: Train[0:1749] Val[1773:3522] Test[3546:5295]\n",
      "  Training h=1... ✓ (OOB: 0.517)\n",
      "  Training h=3... ✓ (OOB: 0.585)\n",
      "  Training h=6... ✓ (OOB: 0.589)\n",
      "\n",
      "============================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Horizon 1:\n",
      "  1. ret_3: 0.1937\n",
      "  2. ret_1: 0.1858\n",
      "  3. vol_6: 0.1674\n",
      "  4. ret_6: 0.1608\n",
      "  5. vol_12: 0.1465\n",
      "  6. ma_ratio: 0.1458\n",
      "  Out-of-bag score: 0.5166\n",
      "\n",
      "Horizon 3:\n",
      "  1. vol_12: 0.1956\n",
      "  2. vol_6: 0.1668\n",
      "  3. ma_ratio: 0.1653\n",
      "  4. ret_1: 0.1626\n",
      "  5. ret_6: 0.1574\n",
      "  6. ret_3: 0.1523\n",
      "  Out-of-bag score: 0.5853\n",
      "\n",
      "Horizon 6:\n",
      "  1. vol_12: 0.2285\n",
      "  2. vol_6: 0.1837\n",
      "  3. ma_ratio: 0.1784\n",
      "  4. ret_6: 0.1631\n",
      "  5. ret_1: 0.1237\n",
      "  6. ret_3: 0.1225\n",
      "  Out-of-bag score: 0.5892\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "Fold 2/3: Train[0:3498] Val[3522:5271] Test[5295:7044]\n",
      "  Training h=1... ✓ (OOB: 0.522)\n",
      "  Training h=3... ✓ (OOB: 0.576)\n",
      "  Training h=6... ✓ (OOB: 0.597)\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "Fold 3/3: Train[0:5247] Val[5271:7020] Test[7044:8747]\n",
      "  Training h=1... ✓ (OOB: 0.545)\n",
      "  Training h=3... ✓ (OOB: 0.570)\n",
      "  Training h=6... ✓ (OOB: 0.587)\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "\n",
      "Completed BTC\n",
      "\n",
      "\n",
      "============================================================\n",
      "RESULTS FOR BTC\n",
      "============================================================\n",
      "\n",
      "Horizon 1:\n",
      "  Test samples: 5198\n",
      "  Mean p_edge: 0.457\n",
      "  Mean mu: 0.0004\n",
      "  Avg Brier: 0.2441\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 79.98%\n",
      "\n",
      "Horizon 3:\n",
      "  Test samples: 5192\n",
      "  Mean p_edge: 0.470\n",
      "  Mean mu: 0.0005\n",
      "  Avg Brier: 0.2458\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 80.01%\n",
      "\n",
      "Horizon 6:\n",
      "  Test samples: 5183\n",
      "  Mean p_edge: 0.487\n",
      "  Mean mu: 0.0005\n",
      "  Avg Brier: 0.2468\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 79.98%\n",
      "\n",
      "============================================================\n",
      "Training Random Forest for ETH\n",
      "Model type: classification\n",
      "Horizons: [1, 3, 6]\n",
      "Folds: 3\n",
      "Trees: 200\n",
      "Max depth: 10\n",
      "============================================================\n",
      "\n",
      "Fold 1/3: Train[0:1749] Val[1773:3522] Test[3546:5295]\n",
      "  Training h=1... ✓ (OOB: 0.494)\n",
      "  Training h=3... ✓ (OOB: 0.558)\n",
      "  Training h=6... ✓ (OOB: 0.624)\n",
      "\n",
      "============================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Horizon 1:\n",
      "  1. ret_1: 0.1798\n",
      "  2. ret_3: 0.1734\n",
      "  3. vol_6: 0.1696\n",
      "  4. ret_6: 0.1656\n",
      "  5. ma_ratio: 0.1583\n",
      "  6. vol_12: 0.1533\n",
      "  Out-of-bag score: 0.4937\n",
      "\n",
      "Horizon 3:\n",
      "  1. ma_ratio: 0.2009\n",
      "  2. vol_12: 0.1748\n",
      "  3. ret_6: 0.1660\n",
      "  4. vol_6: 0.1641\n",
      "  5. ret_3: 0.1494\n",
      "  6. ret_1: 0.1448\n",
      "  Out-of-bag score: 0.5578\n",
      "\n",
      "Horizon 6:\n",
      "  1. vol_12: 0.1965\n",
      "  2. ma_ratio: 0.1867\n",
      "  3. vol_6: 0.1780\n",
      "  4. ret_6: 0.1598\n",
      "  5. ret_1: 0.1436\n",
      "  6. ret_3: 0.1353\n",
      "  Out-of-bag score: 0.6242\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "Fold 2/3: Train[0:3498] Val[3522:5271] Test[5295:7044]\n",
      "  Training h=1... ✓ (OOB: 0.541)\n",
      "  Training h=3... ✓ (OOB: 0.560)\n",
      "  Training h=6... ✓ (OOB: 0.593)\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "Fold 3/3: Train[0:5247] Val[5271:7020] Test[7044:8747]\n",
      "  Training h=1... ✓ (OOB: 0.544)\n",
      "  Training h=3... ✓ (OOB: 0.551)\n",
      "  Training h=6... ✓ (OOB: 0.570)\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "\n",
      "Completed ETH\n",
      "\n",
      "\n",
      "============================================================\n",
      "RESULTS FOR ETH\n",
      "============================================================\n",
      "\n",
      "Horizon 1:\n",
      "  Test samples: 5198\n",
      "  Mean p_edge: 0.461\n",
      "  Mean mu: 0.0005\n",
      "  Avg Brier: 0.2453\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 79.98%\n",
      "\n",
      "Horizon 3:\n",
      "  Test samples: 5192\n",
      "  Mean p_edge: 0.471\n",
      "  Mean mu: 0.0004\n",
      "  Avg Brier: 0.2471\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 80.01%\n",
      "\n",
      "Horizon 6:\n",
      "  Test samples: 5183\n",
      "  Mean p_edge: 0.486\n",
      "  Mean mu: 0.0004\n",
      "  Avg Brier: 0.2480\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 79.98%\n",
      "\n",
      "============================================================\n",
      "Training Random Forest for SOL\n",
      "Model type: classification\n",
      "Horizons: [1, 3, 6]\n",
      "Folds: 3\n",
      "Trees: 200\n",
      "Max depth: 10\n",
      "============================================================\n",
      "\n",
      "Fold 1/3: Train[0:1749] Val[1773:3522] Test[3546:5295]\n",
      "  Training h=1... ✓ (OOB: 0.487)\n",
      "  Training h=3... ✓ (OOB: 0.568)\n",
      "  Training h=6... ✓ (OOB: 0.615)\n",
      "\n",
      "============================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Horizon 1:\n",
      "  1. ret_3: 0.1793\n",
      "  2. vol_6: 0.1761\n",
      "  3. ret_6: 0.1696\n",
      "  4. ret_1: 0.1682\n",
      "  5. ma_ratio: 0.1580\n",
      "  6. vol_12: 0.1488\n",
      "  Out-of-bag score: 0.4868\n",
      "\n",
      "Horizon 3:\n",
      "  1. ma_ratio: 0.2069\n",
      "  2. vol_6: 0.1833\n",
      "  3. vol_12: 0.1678\n",
      "  4. ret_6: 0.1652\n",
      "  5. ret_3: 0.1390\n",
      "  6. ret_1: 0.1379\n",
      "  Out-of-bag score: 0.5676\n",
      "\n",
      "Horizon 6:\n",
      "  1. vol_12: 0.2019\n",
      "  2. ma_ratio: 0.1901\n",
      "  3. vol_6: 0.1882\n",
      "  4. ret_6: 0.1603\n",
      "  5. ret_3: 0.1369\n",
      "  6. ret_1: 0.1226\n",
      "  Out-of-bag score: 0.6150\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "Fold 2/3: Train[0:3498] Val[3522:5271] Test[5295:7044]\n",
      "  Training h=1... ✓ (OOB: 0.519)\n",
      "  Training h=3... ✓ (OOB: 0.559)\n",
      "  Training h=6... ✓ (OOB: 0.577)\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "Fold 3/3: Train[0:5247] Val[5271:7020] Test[7044:8747]\n",
      "  Training h=1... ✓ (OOB: 0.517)\n",
      "  Training h=3... ✓ (OOB: 0.548)\n",
      "  Training h=6... ✓ (OOB: 0.563)\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "\n",
      "Completed SOL\n",
      "\n",
      "\n",
      "============================================================\n",
      "RESULTS FOR SOL\n",
      "============================================================\n",
      "\n",
      "Horizon 1:\n",
      "  Test samples: 5198\n",
      "  Mean p_edge: 0.484\n",
      "  Mean mu: 0.0004\n",
      "  Avg Brier: 0.2473\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 79.98%\n",
      "\n",
      "Horizon 3:\n",
      "  Test samples: 5192\n",
      "  Mean p_edge: 0.482\n",
      "  Mean mu: 0.0004\n",
      "  Avg Brier: 0.2471\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 80.01%\n",
      "\n",
      "Horizon 6:\n",
      "  Test samples: 5183\n",
      "  Mean p_edge: 0.482\n",
      "  Mean mu: 0.0003\n",
      "  Avg Brier: 0.2465\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 79.98%\n",
      "\n",
      "============================================================\n",
      "Training Random Forest for XRP\n",
      "Model type: classification\n",
      "Horizons: [1, 3, 6]\n",
      "Folds: 3\n",
      "Trees: 200\n",
      "Max depth: 10\n",
      "============================================================\n",
      "\n",
      "Fold 1/3: Train[0:1749] Val[1773:3522] Test[3546:5295]\n",
      "  Training h=1... ✓ (OOB: 0.505)\n",
      "  Training h=3... ✓ (OOB: 0.589)\n",
      "  Training h=6... ✓ (OOB: 0.585)\n",
      "\n",
      "============================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Horizon 1:\n",
      "  1. ret_3: 0.2005\n",
      "  2. ret_6: 0.1867\n",
      "  3. vol_6: 0.1638\n",
      "  4. ret_1: 0.1567\n",
      "  5. ma_ratio: 0.1504\n",
      "  6. vol_12: 0.1419\n",
      "  Out-of-bag score: 0.5051\n",
      "\n",
      "Horizon 3:\n",
      "  1. ret_6: 0.1945\n",
      "  2. ma_ratio: 0.1863\n",
      "  3. vol_6: 0.1642\n",
      "  4. vol_12: 0.1621\n",
      "  5. ret_3: 0.1568\n",
      "  6. ret_1: 0.1360\n",
      "  Out-of-bag score: 0.5888\n",
      "\n",
      "Horizon 6:\n",
      "  1. ma_ratio: 0.1885\n",
      "  2. vol_12: 0.1824\n",
      "  3. ret_6: 0.1790\n",
      "  4. ret_3: 0.1747\n",
      "  5. vol_6: 0.1621\n",
      "  6. ret_1: 0.1133\n",
      "  Out-of-bag score: 0.5852\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "Fold 2/3: Train[0:3498] Val[3522:5271] Test[5295:7044]\n",
      "  Training h=1... ✓ (OOB: 0.516)\n",
      "  Training h=3... ✓ (OOB: 0.566)\n",
      "  Training h=6... ✓ (OOB: 0.588)\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "Fold 3/3: Train[0:5247] Val[5271:7020] Test[7044:8747]\n",
      "  Training h=1... ✓ (OOB: 0.538)\n",
      "  Training h=3... ✓ (OOB: 0.560)\n",
      "  Training h=6... ✓ (OOB: 0.575)\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "\n",
      "Completed XRP\n",
      "\n",
      "\n",
      "============================================================\n",
      "RESULTS FOR XRP\n",
      "============================================================\n",
      "\n",
      "Horizon 1:\n",
      "  Test samples: 5198\n",
      "  Mean p_edge: 0.474\n",
      "  Mean mu: 0.0005\n",
      "  Avg Brier: 0.2452\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 79.98%\n",
      "\n",
      "Horizon 3:\n",
      "  Test samples: 5192\n",
      "  Mean p_edge: 0.481\n",
      "  Mean mu: 0.0004\n",
      "  Avg Brier: 0.2469\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 80.01%\n",
      "\n",
      "Horizon 6:\n",
      "  Test samples: 5183\n",
      "  Mean p_edge: 0.482\n",
      "  Mean mu: 0.0004\n",
      "  Avg Brier: 0.2444\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 79.98%\n",
      "\n",
      "============================================================\n",
      "Training Random Forest for DOGE\n",
      "Model type: classification\n",
      "Horizons: [1, 3, 6]\n",
      "Folds: 3\n",
      "Trees: 200\n",
      "Max depth: 10\n",
      "============================================================\n",
      "\n",
      "Fold 1/3: Train[0:1749] Val[1773:3522] Test[3546:5295]\n",
      "  Training h=1... ✓ (OOB: 0.508)\n",
      "  Training h=3... ✓ (OOB: 0.578)\n",
      "  Training h=6... ✓ (OOB: 0.608)\n",
      "\n",
      "============================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Horizon 1:\n",
      "  1. ret_6: 0.1963\n",
      "  2. ret_1: 0.1708\n",
      "  3. vol_6: 0.1656\n",
      "  4. vol_12: 0.1611\n",
      "  5. ma_ratio: 0.1576\n",
      "  6. ret_3: 0.1485\n",
      "  Out-of-bag score: 0.5080\n",
      "\n",
      "Horizon 3:\n",
      "  1. ret_6: 0.1962\n",
      "  2. ma_ratio: 0.1772\n",
      "  3. vol_12: 0.1684\n",
      "  4. vol_6: 0.1598\n",
      "  5. ret_1: 0.1498\n",
      "  6. ret_3: 0.1486\n",
      "  Out-of-bag score: 0.5785\n",
      "\n",
      "Horizon 6:\n",
      "  1. ret_6: 0.2063\n",
      "  2. ma_ratio: 0.1835\n",
      "  3. vol_12: 0.1801\n",
      "  4. vol_6: 0.1732\n",
      "  5. ret_3: 0.1427\n",
      "  6. ret_1: 0.1142\n",
      "  Out-of-bag score: 0.6076\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "Fold 2/3: Train[0:3498] Val[3522:5271] Test[5295:7044]\n",
      "  Training h=1... ✓ (OOB: 0.528)\n",
      "  Training h=3... ✓ (OOB: 0.569)\n",
      "  Training h=6... ✓ (OOB: 0.593)\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "Fold 3/3: Train[0:5247] Val[5271:7020] Test[7044:8747]\n",
      "  Training h=1... ✓ (OOB: 0.545)\n",
      "  Training h=3... ✓ (OOB: 0.565)\n",
      "  Training h=6... ✓ (OOB: 0.587)\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "\n",
      "Completed DOGE\n",
      "\n",
      "\n",
      "============================================================\n",
      "RESULTS FOR DOGE\n",
      "============================================================\n",
      "\n",
      "Horizon 1:\n",
      "  Test samples: 5198\n",
      "  Mean p_edge: 0.479\n",
      "  Mean mu: 0.0004\n",
      "  Avg Brier: 0.2449\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 79.98%\n",
      "\n",
      "Horizon 3:\n",
      "  Test samples: 5192\n",
      "  Mean p_edge: 0.486\n",
      "  Mean mu: 0.0004\n",
      "  Avg Brier: 0.2459\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 80.01%\n",
      "\n",
      "Horizon 6:\n",
      "  Test samples: 5183\n",
      "  Mean p_edge: 0.490\n",
      "  Mean mu: 0.0004\n",
      "  Avg Brier: 0.2461\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 79.98%\n",
      "\n",
      "============================================================\n",
      "Wrote 77865 records to: /Users/nitinlodha/Desktop/ML/ML_Project/Bybit_CSV_Data/trader_feed_rf_multiH.jsonl\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# MAIN EXECUTION\n",
    "# =========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Process all symbols\n",
    "    all_outputs = {}\n",
    "\n",
    "    for symbol, path in FILES.items():\n",
    "        if not path.exists():\n",
    "            print(f\"Warning: {path} not found, skipping {symbol}\")\n",
    "            continue\n",
    "\n",
    "        results = run_rf_for_symbol(\n",
    "            symbol=symbol,\n",
    "            path=path,\n",
    "            horizons=HORIZONS,\n",
    "            n_folds=3,\n",
    "            embargo=24,\n",
    "            model_type='classification',  # or 'regression'\n",
    "            n_estimators=200,  # More trees = better but slower\n",
    "            max_depth=10,  # Deeper = more complex (risk overfitting)\n",
    "            compute_tree_variance=True  # Use tree variance for uncertainty\n",
    "        )\n",
    "\n",
    "        all_outputs[symbol] = results\n",
    "\n",
    "        # Print summary\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"RESULTS FOR {symbol}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        for h in HORIZONS:\n",
    "            test_df = results[h]['test']\n",
    "            diag_list = results[h]['diag']\n",
    "\n",
    "            if len(test_df) > 0:\n",
    "                print(f\"\\nHorizon {h}:\")\n",
    "                print(f\"  Test samples: {len(test_df)}\")\n",
    "                print(f\"  Mean p_edge: {test_df['p_edge'].mean():.3f}\")\n",
    "                print(f\"  Mean mu: {test_df['mu'].mean():.4f}\")\n",
    "\n",
    "                if diag_list:\n",
    "                    avg_brier = np.mean([d['brier_val'] for d in diag_list])\n",
    "                    avg_ece = np.mean([d['ece_val'] for d in diag_list])\n",
    "                    avg_cov = np.mean([d['pi_coverage_val'] for d in diag_list])\n",
    "                    print(f\"  Avg Brier: {avg_brier:.4f}\")\n",
    "                    print(f\"  Avg ECE: {avg_ece:.4f}\")\n",
    "                    print(f\"  Avg PI coverage: {avg_cov:.2%}\")\n",
    "\n",
    "    # Export to JSON\n",
    "    json_records = build_json_records(all_outputs)\n",
    "    json_path = DATA_DIR / \"trader_feed_rf_multiH.jsonl\"\n",
    "    with open(json_path, \"w\") as f:\n",
    "        for r in json_records:\n",
    "            f.write(json.dumps(r) + \"\\n\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Wrote {len(json_records)} records to: {json_path}\")\n",
    "    print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e269579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
