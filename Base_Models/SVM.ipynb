{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46b3f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c7f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"/Users/nitinlodha/Desktop/ML/ML_Project/Bybit_CSV_Data\")\n",
    "FILES = {\n",
    "    \"BTC\": DATA_DIR / \"Bybit_BTC.csv\",\n",
    "    \"ETH\": DATA_DIR / \"Bybit_ETH.csv\",\n",
    "    \"SOL\": DATA_DIR / \"Bybit_SOL.csv\",\n",
    "    \"XRP\": DATA_DIR / \"Bybit_XRP.csv\",\n",
    "    \"DOGE\": DATA_DIR / \"Bybit_DOGE.csv\",\n",
    "}\n",
    "\n",
    "HORIZONS = [1, 3, 6]\n",
    "DEFAULT_COST_BP = {1: 8.0, 3: 10.0, 6: 12.0}\n",
    "\n",
    "def bp_to_logret(bp: float) -> float:\n",
    "    return bp * 1e-4\n",
    "\n",
    "# Policy thresholds\n",
    "TAU_P = 0.60\n",
    "TAU_MU = 0.0005\n",
    "LAM = 2.0\n",
    "W_MAX = 0.50\n",
    "\n",
    "MODEL_VERSION = \"svm_rbf_multiH_v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee50ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_close_column(df: pd.DataFrame) -> str:\n",
    "    lower = {c.lower(): c for c in df.columns}\n",
    "    for key in (\"close\", \"closing_price\", \"close_price\", \"price_close\", \"last\", \"c\"):\n",
    "        if key in lower:\n",
    "            return lower[key]\n",
    "    float_cols = [c for c in df.columns if pd.api.types.is_float_dtype(df[c])]\n",
    "    if len(float_cols) == 1:\n",
    "        return float_cols[0]\n",
    "    raise ValueError(\"Cannot identify 'close' column.\")\n",
    "\n",
    "def make_feature_table(close: pd.Series):\n",
    "    \"\"\"Build feature table from close prices.\"\"\"\n",
    "    df = pd.DataFrame(index=close.index)\n",
    "    df[\"price\"] = close.astype(float)\n",
    "\n",
    "    # Returns at different lags\n",
    "    df[\"ret_1\"] = np.log(df[\"price\"] / df[\"price\"].shift(1))\n",
    "    df[\"ret_3\"] = np.log(df[\"price\"] / df[\"price\"].shift(3))\n",
    "    df[\"ret_6\"] = np.log(df[\"price\"] / df[\"price\"].shift(6))\n",
    "\n",
    "    # Volatility\n",
    "    df[\"vol_6\"] = df[\"ret_1\"].rolling(6).std()\n",
    "    df[\"vol_12\"] = df[\"ret_1\"].rolling(12).std()\n",
    "\n",
    "    # Moving averages (log ratio)\n",
    "    ma_10 = df[\"price\"].rolling(10).mean()\n",
    "    ma_20 = df[\"price\"].rolling(20).mean()\n",
    "    df[\"ma_ratio\"] = np.log(ma_10 / ma_20)\n",
    "\n",
    "    # Drop NaN rows\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Feature matrix (exclude price itself)\n",
    "    feat_cols = [c for c in df.columns if c != \"price\"]\n",
    "    X = df[feat_cols].values.astype(float)\n",
    "\n",
    "    return df, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fd3f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SVMSnapshot:\n",
    "    \"\"\"Holds trained SVM and preprocessing for one horizon\"\"\"\n",
    "    clf: SVC | SVR\n",
    "    scaler: StandardScaler\n",
    "    model_type: str  # 'classification' or 'regression'\n",
    "    horizon: int\n",
    "    C: float = 1.0\n",
    "    gamma: float | str = 'scale'\n",
    "\n",
    "def fit_svm_classifier(X_train: np.ndarray, y_train: np.ndarray,\n",
    "                       horizon: int, random_state: int = 123,\n",
    "                       C: float = 1.0, gamma: float | str = 'scale') -> SVMSnapshot:\n",
    "    \"\"\"\n",
    "    Train calibrated SVM classifier for binary edge detection.\n",
    "\n",
    "    Args:\n",
    "        X_train: Feature matrix (T, D)\n",
    "        y_train: Binary labels, 1 if return > cost, else 0\n",
    "        horizon: Forecast horizon in bars\n",
    "        C: SVM regularization parameter\n",
    "        gamma: RBF kernel parameter\n",
    "\n",
    "    Returns:\n",
    "        SVMSnapshot with calibrated classifier\n",
    "    \"\"\"\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_scaled = scaler.transform(X_train)\n",
    "\n",
    "    # Base SVM with RBF kernel\n",
    "    base_svm = SVC(\n",
    "        kernel='rbf',\n",
    "        C=C,\n",
    "        gamma=gamma,\n",
    "        class_weight='balanced',  # Handle imbalanced data\n",
    "        random_state=random_state,\n",
    "        probability=False,  # Calibration will provide probabilities\n",
    "        cache_size=500  # MB for kernel cache\n",
    "    )\n",
    "\n",
    "    # Calibrate probabilities using isotonic regression\n",
    "    # This is CRITICAL for reliable P(edge > cost) estimates\n",
    "    clf = CalibratedClassifierCV(\n",
    "        base_svm,\n",
    "        method='isotonic',  # More flexible than 'sigmoid'\n",
    "        cv=3,  # 3-fold cross-validation for calibration\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    clf.fit(X_scaled, y_train)\n",
    "\n",
    "    return SVMSnapshot(\n",
    "        clf=clf,\n",
    "        scaler=scaler,\n",
    "        model_type='classification',\n",
    "        horizon=horizon,\n",
    "        C=C,\n",
    "        gamma=gamma\n",
    "    )\n",
    "\n",
    "def fit_svm_regressor(X_train: np.ndarray, y_train: np.ndarray,\n",
    "                      horizon: int, random_state: int = 123,\n",
    "                      C: float = 1.0, gamma: float | str = 'scale',\n",
    "                      epsilon: float = 0.1) -> SVMSnapshot:\n",
    "    \"\"\"\n",
    "    Train SVR to directly predict continuous returns.\n",
    "\n",
    "    Args:\n",
    "        X_train: Feature matrix\n",
    "        y_train: Continuous log returns\n",
    "        epsilon: Epsilon-insensitive loss parameter\n",
    "\n",
    "    Returns:\n",
    "        SVMSnapshot with trained regressor\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_scaled = scaler.transform(X_train)\n",
    "\n",
    "    reg = SVR(\n",
    "        kernel='rbf',\n",
    "        C=C,\n",
    "        gamma=gamma,\n",
    "        epsilon=epsilon,\n",
    "        cache_size=500\n",
    "    )\n",
    "\n",
    "    reg.fit(X_scaled, y_train)\n",
    "\n",
    "    return SVMSnapshot(\n",
    "        clf=reg,\n",
    "        scaler=scaler,\n",
    "        model_type='regression',\n",
    "        horizon=horizon,\n",
    "        C=C,\n",
    "        gamma=gamma\n",
    "    )\n",
    "\n",
    "def forecast_multi_horizon_svm(\n",
    "    snapshots: dict[int, SVMSnapshot],\n",
    "    X_seg: np.ndarray,\n",
    "    price_seg: pd.Series,\n",
    "    horizons: list[int],\n",
    "    cost_bp: dict[int, float] | None = None,\n",
    "    n_bootstrap: int = 0  # Set to 0 to disable bootstrap (faster)\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate multi-horizon forecasts using trained SVMs.\n",
    "\n",
    "    Key differences from HMM approach:\n",
    "    - Direct prediction from features (no state transitions)\n",
    "    - One model per horizon\n",
    "    - Bootstrap resampling for uncertainty (optional)\n",
    "\n",
    "    Args:\n",
    "        snapshots: Dict mapping horizon -> trained SVMSnapshot\n",
    "        X_seg: Feature matrix for forecast segment\n",
    "        price_seg: Corresponding price series\n",
    "        horizons: List of forecast horizons\n",
    "        cost_bp: Trading costs in basis points\n",
    "        n_bootstrap: Number of bootstrap samples (0 = disabled)\n",
    "\n",
    "    Returns:\n",
    "        out: dict[horizon] -> DataFrame with predictions\n",
    "        cost_log: dict[horizon] -> cost in log-return units\n",
    "    \"\"\"\n",
    "    if cost_bp is None:\n",
    "        cost_bp = {h: DEFAULT_COST_BP.get(h, 8.0) for h in horizons}\n",
    "    cost_log = {h: bp_to_logret(float(cost_bp[h])) for h in horizons}\n",
    "\n",
    "    Tseg = X_seg.shape[0]\n",
    "    idx = price_seg.index\n",
    "    out = {}\n",
    "\n",
    "    for h in horizons:\n",
    "        if h not in snapshots:\n",
    "            print(f\"Warning: No model for horizon {h}, skipping\")\n",
    "            continue\n",
    "\n",
    "        snap = snapshots[h]\n",
    "        out_h = pd.DataFrame(index=idx[:-h] if h < Tseg else idx[:0])\n",
    "        T_h = Tseg - h\n",
    "\n",
    "        if T_h <= 0:\n",
    "            out[h] = out_h\n",
    "            continue\n",
    "\n",
    "        # Scale features\n",
    "        X_scaled = snap.scaler.transform(X_seg[:T_h])\n",
    "\n",
    "        if snap.model_type == 'classification':\n",
    "            # ========================================\n",
    "            # Classification: Predict P(return > cost)\n",
    "            # ========================================\n",
    "\n",
    "            # Get calibrated probabilities\n",
    "            p_edge = snap.clf.predict_proba(X_scaled)[:, 1]  # P(positive class)\n",
    "\n",
    "            # Expected return estimation\n",
    "            # Option 1: Threshold-based (simple)\n",
    "            y_pred = snap.clf.predict(X_scaled)\n",
    "            mu = np.where(y_pred == 1,\n",
    "                         cost_log[h] + 0.002,  # Small positive edge\n",
    "                         -cost_log[h] - 0.001)  # Small negative edge\n",
    "\n",
    "            # Option 2: Probability-weighted (better)\n",
    "            # mu = p_edge * (cost_log[h] + 0.002) + (1 - p_edge) * (-cost_log[h] - 0.001)\n",
    "\n",
    "            # Uncertainty via bootstrap (optional)\n",
    "            if n_bootstrap > 0:\n",
    "                std_h, q10, q50, q90 = _bootstrap_uncertainty_classification(\n",
    "                    snap, X_scaled, n_bootstrap, cost_log[h]\n",
    "                )\n",
    "            else:\n",
    "                std_h = np.full(T_h, 0.01)  # Default uncertainty\n",
    "                q10 = mu - 0.02\n",
    "                q50 = mu\n",
    "                q90 = mu + 0.02\n",
    "\n",
    "        elif snap.model_type == 'regression':\n",
    "            # ========================================\n",
    "            # Regression: Predict continuous returns\n",
    "            # ========================================\n",
    "\n",
    "            mu = snap.clf.predict(X_scaled)\n",
    "\n",
    "            # Probability via sigmoid transform\n",
    "            p_edge = 1.0 / (1.0 + np.exp(-10 * (mu - cost_log[h])))\n",
    "\n",
    "            if n_bootstrap > 0:\n",
    "                std_h, q10, q50, q90 = _bootstrap_uncertainty_regression(\n",
    "                    snap, X_scaled, n_bootstrap\n",
    "                )\n",
    "            else:\n",
    "                std_h = np.full(T_h, 0.015)\n",
    "                q10 = mu - 0.025\n",
    "                q50 = mu\n",
    "                q90 = mu + 0.025\n",
    "\n",
    "        # Populate DataFrame\n",
    "        p_now = price_seg.iloc[:T_h].values\n",
    "\n",
    "        out_h['mu'] = mu\n",
    "        out_h['std'] = std_h\n",
    "        out_h['p_edge_raw'] = p_edge\n",
    "        out_h['ret_q10'] = q10\n",
    "        out_h['ret_q50'] = q50\n",
    "        out_h['ret_q90'] = q90\n",
    "        out_h['price_pred'] = p_now * np.exp(mu)\n",
    "        out_h['price_q10'] = p_now * np.exp(q10)\n",
    "        out_h['price_q50'] = p_now * np.exp(q50)\n",
    "        out_h['price_q90'] = p_now * np.exp(q90)\n",
    "\n",
    "        out[h] = out_h\n",
    "\n",
    "    return out, cost_log\n",
    "\n",
    "def _bootstrap_uncertainty_classification(snap, X_scaled, n_bootstrap, cost_threshold):\n",
    "    \"\"\"Bootstrap resampling for classification uncertainty.\"\"\"\n",
    "    T = len(X_scaled)\n",
    "    rng = np.random.default_rng(42)\n",
    "    probs = []\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        boot_idx = rng.choice(T, size=T, replace=True)\n",
    "        X_boot = X_scaled[boot_idx]\n",
    "        p_boot = snap.clf.predict_proba(X_boot)[:, 1]\n",
    "        probs.append(p_boot[:T])\n",
    "\n",
    "    probs = np.array(probs)\n",
    "    std = np.std(probs, axis=0)\n",
    "\n",
    "    # Convert probabilities to return estimates for quantiles\n",
    "    # This is approximate - better to train a separate regressor\n",
    "    mu_samples = probs * (cost_threshold + 0.002) + (1 - probs) * (-cost_threshold - 0.001)\n",
    "\n",
    "    q10 = np.percentile(mu_samples, 10, axis=0)\n",
    "    q50 = np.percentile(mu_samples, 50, axis=0)\n",
    "    q90 = np.percentile(mu_samples, 90, axis=0)\n",
    "\n",
    "    return std, q10, q50, q90\n",
    "\n",
    "def _bootstrap_uncertainty_regression(snap, X_scaled, n_bootstrap):\n",
    "    \"\"\"Bootstrap resampling for regression uncertainty.\"\"\"\n",
    "    T = len(X_scaled)\n",
    "    rng = np.random.default_rng(42)\n",
    "    preds = []\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        boot_idx = rng.choice(T, size=T, replace=True)\n",
    "        X_boot = X_scaled[boot_idx]\n",
    "        pred_boot = snap.clf.predict(X_boot)\n",
    "        preds.append(pred_boot[:T])\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    std = np.std(preds, axis=0)\n",
    "    q10 = np.percentile(preds, 10, axis=0)\n",
    "    q50 = np.percentile(preds, 50, axis=0)\n",
    "    q90 = np.percentile(preds, 90, axis=0)\n",
    "\n",
    "    return std, q10, q50, q90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dce05f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProbCalibrator:\n",
    "    method: str\n",
    "    iso: IsotonicRegression | None = None\n",
    "\n",
    "def fit_prob_calibrator_isotonic(p_raw: np.ndarray, y: np.ndarray,\n",
    "                                 min_points: int = 30) -> ProbCalibrator:\n",
    "    p_raw = np.asarray(p_raw, float)\n",
    "    y = np.asarray(y, float)\n",
    "    m = np.isfinite(p_raw) & np.isfinite(y)\n",
    "    p, t = p_raw[m], y[m]\n",
    "    if p.size < min_points or np.unique(p).size < 3:\n",
    "        return ProbCalibrator(method=\"identity\", iso=None)\n",
    "    iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "    iso.fit(p, t)\n",
    "    return ProbCalibrator(method=\"isotonic\", iso=iso)\n",
    "\n",
    "def apply_prob_calibrator(cal: ProbCalibrator, p_raw: np.ndarray) -> np.ndarray:\n",
    "    p_raw = np.asarray(p_raw, float)\n",
    "    if cal.method == \"isotonic\":\n",
    "        return cal.iso.predict(p_raw)\n",
    "    return p_raw\n",
    "\n",
    "@dataclass\n",
    "class IntervalCalibrator:\n",
    "    method: str\n",
    "    q_alpha: float\n",
    "    alpha: float\n",
    "\n",
    "def fit_conformal_interval(residuals: np.ndarray, alpha: float = 0.2) -> IntervalCalibrator:\n",
    "    resid = np.asarray(residuals, float)\n",
    "    resid = resid[np.isfinite(resid)]\n",
    "    q = float(np.quantile(np.abs(resid), 1 - alpha)) if resid.size > 0 else 0.0\n",
    "    return IntervalCalibrator(method=\"conformal_abs\", q_alpha=q, alpha=alpha)\n",
    "\n",
    "def apply_conformal_interval(cal: IntervalCalibrator, mu: np.ndarray):\n",
    "    mu = np.asarray(mu, float)\n",
    "    return mu - cal.q_alpha, mu + cal.q_alpha\n",
    "\n",
    "def cumulative_log_returns(price: pd.Series, h: int) -> pd.Series:\n",
    "    return np.log(price.shift(-h) / price).dropna()\n",
    "\n",
    "def brier_score(y: np.ndarray, p: np.ndarray) -> float:\n",
    "    return float(np.mean((y - p) ** 2))\n",
    "\n",
    "def expected_calibration_error(y: np.ndarray, p: np.ndarray, bins: int = 10) -> float:\n",
    "    edges = np.linspace(0, 1, bins + 1)\n",
    "    ece = 0.0\n",
    "    for i in range(bins):\n",
    "        m = (p >= edges[i]) & (p < edges[i+1])\n",
    "        if m.sum() == 0:\n",
    "            continue\n",
    "        ece += (m.sum()/len(p)) * np.abs(np.mean(y[m]) - np.mean(p[m]))\n",
    "    return float(ece)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88135313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def purged_walkforward_slices(n: int, n_folds: int = 3, embargo: int = 24):\n",
    "    \"\"\"Generate (train, val, test) slices for walk-forward CV.\"\"\"\n",
    "    fold_size = n // (n_folds + 2)\n",
    "    slices = []\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        train_end = (i + 1) * fold_size\n",
    "        val_start = train_end + embargo\n",
    "        val_end = val_start + fold_size\n",
    "        test_start = val_end + embargo\n",
    "        test_end = min(test_start + fold_size, n)\n",
    "\n",
    "        if test_end - test_start < fold_size // 2:\n",
    "            break\n",
    "\n",
    "        slices.append((\n",
    "            (0, train_end),\n",
    "            (val_start, val_end),\n",
    "            (test_start, test_end)\n",
    "        ))\n",
    "\n",
    "    return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d2ce95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_partA_for_symbol(symbol: str, path: Path,\n",
    "                         horizons: list[int] = HORIZONS,\n",
    "                         n_folds: int = 3,\n",
    "                         embargo: int = 24,\n",
    "                         model_type: str = 'classification',\n",
    "                         n_bootstrap: int = 0):\n",
    "    \"\"\"\n",
    "    Train and evaluate SVM models for one symbol.\n",
    "\n",
    "    Args:\n",
    "        symbol: Asset symbol\n",
    "        path: Path to CSV file\n",
    "        horizons: Forecast horizons in bars\n",
    "        n_folds: Number of walk-forward folds\n",
    "        embargo: Embargo period between folds\n",
    "        model_type: 'classification' or 'regression'\n",
    "        n_bootstrap: Bootstrap samples for uncertainty (0 = disabled)\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df_raw = pd.read_csv(path)\n",
    "    close_col = _find_close_column(df_raw)\n",
    "    close = pd.Series(df_raw[close_col].astype(float).values,\n",
    "                      index=pd.RangeIndex(len(df_raw)), name=\"close\")\n",
    "\n",
    "    feat_df, X = make_feature_table(close)\n",
    "    price = feat_df[\"price\"]\n",
    "    n = len(price)\n",
    "\n",
    "    folds = purged_walkforward_slices(n, n_folds=n_folds, embargo=embargo)\n",
    "\n",
    "    results = {h: {\"val\": [], \"test\": [], \"diag\": []} for h in horizons}\n",
    "\n",
    "    print(f\"\\\\n{'='*60}\")\n",
    "    print(f\"Training SVM for {symbol}\")\n",
    "    print(f\"Model type: {model_type}\")\n",
    "    print(f\"Horizons: {horizons}\")\n",
    "    print(f\"Folds: {n_folds}\")\n",
    "    print(f\"{'='*60}\\\\n\")\n",
    "\n",
    "    for fold_idx, ((s0,e0), (s1,e1), (s2,e2)) in enumerate(folds):\n",
    "        print(f\"Fold {fold_idx + 1}/{len(folds)}: Train[{s0}:{e0}] Val[{s1}:{e1}] Test[{s2}:{e2}]\")\n",
    "\n",
    "        # ========================================\n",
    "        # KEY CHANGE: Train one SVM per horizon\n",
    "        # ========================================\n",
    "        snapshots = {}\n",
    "\n",
    "        for h in horizons:\n",
    "            print(f\"  Training h={h}...\", end=\" \")\n",
    "\n",
    "            # Create labels for this horizon\n",
    "            ret_train = cumulative_log_returns(price.iloc[s0:e0], h)\n",
    "\n",
    "            # Align features and labels\n",
    "            n_train = min(len(X[s0:e0]), len(ret_train))\n",
    "            X_train_aligned = X[s0:s0+n_train]\n",
    "            ret_train_aligned = ret_train.iloc[:n_train]\n",
    "\n",
    "            if len(X_train_aligned) < 50:\n",
    "                print(\"SKIP (insufficient data)\")\n",
    "                continue\n",
    "\n",
    "            if model_type == 'classification':\n",
    "                # Binary classification: profitable vs not\n",
    "                y_train = (ret_train_aligned.values > bp_to_logret(DEFAULT_COST_BP[h])).astype(int)\n",
    "\n",
    "                # Check class balance\n",
    "                pos_frac = y_train.mean()\n",
    "                if pos_frac < 0.1 or pos_frac > 0.9:\n",
    "                    print(f\"WARN (imbalanced: {pos_frac:.2%} positive)\")\n",
    "\n",
    "                snap = fit_svm_classifier(\n",
    "                    X_train_aligned, y_train,\n",
    "                    horizon=h,\n",
    "                    random_state=123 + fold_idx,\n",
    "                    C=10.0,  # Can tune this\n",
    "                    gamma='scale'\n",
    "                )\n",
    "            else:\n",
    "                # Regression: predict actual returns\n",
    "                y_train = ret_train_aligned.values\n",
    "                snap = fit_svm_regressor(\n",
    "                    X_train_aligned, y_train,\n",
    "                    horizon=h,\n",
    "                    random_state=123 + fold_idx,\n",
    "                    C=10.0,\n",
    "                    gamma='scale',\n",
    "                    epsilon=0.01\n",
    "                )\n",
    "\n",
    "            snapshots[h] = snap\n",
    "            print(\"✓\")\n",
    "\n",
    "        if not snapshots:\n",
    "            print(\"  No models trained, skipping fold\")\n",
    "            continue\n",
    "\n",
    "        # Forecast on validation and test\n",
    "        print(\"  Forecasting validation...\", end=\" \")\n",
    "        out_val_raw, cost_log = forecast_multi_horizon_svm(\n",
    "            snapshots=snapshots,\n",
    "            X_seg=X[s1:e1],\n",
    "            price_seg=price.iloc[s1:e1],\n",
    "            horizons=horizons,\n",
    "            n_bootstrap=n_bootstrap\n",
    "        )\n",
    "        print(\"✓\")\n",
    "\n",
    "        print(\"  Forecasting test...\", end=\" \")\n",
    "        out_test_raw, _ = forecast_multi_horizon_svm(\n",
    "            snapshots=snapshots,\n",
    "            X_seg=X[s2:e2],\n",
    "            price_seg=price.iloc[s2:e2],\n",
    "            horizons=horizons,\n",
    "            n_bootstrap=n_bootstrap\n",
    "        )\n",
    "        print(\"✓\")\n",
    "\n",
    "        # Calibration (same as HMM version)\n",
    "        for h in horizons:\n",
    "            if h not in out_val_raw or h not in out_test_raw:\n",
    "                continue\n",
    "\n",
    "            ret_val = cumulative_log_returns(price.iloc[s1:e1], h)\n",
    "            idx_common = out_val_raw[h].index.intersection(ret_val.index)\n",
    "\n",
    "            if len(idx_common) == 0:\n",
    "                continue\n",
    "\n",
    "            dfV = out_val_raw[h].loc[idx_common].copy()\n",
    "            maskV = np.isfinite(dfV[\"p_edge_raw\"].values) & np.isfinite(dfV[\"mu\"].values)\n",
    "            dfV = dfV[maskV]\n",
    "\n",
    "            if len(dfV) < 20:\n",
    "                continue\n",
    "\n",
    "            ret_val_aligned = ret_val.loc[dfV.index]\n",
    "            y_val = (ret_val_aligned.values > cost_log[h]).astype(int)\n",
    "            p_raw_val = dfV[\"p_edge_raw\"].values\n",
    "            mu_val = dfV[\"mu\"].values\n",
    "\n",
    "            # Fit calibrators\n",
    "            cal_prob = fit_prob_calibrator_isotonic(p_raw_val, y_val, min_points=20)\n",
    "            resid_val = ret_val_aligned.values - mu_val\n",
    "            cal_pi = fit_conformal_interval(resid_val, alpha=0.2)\n",
    "\n",
    "            # Apply to test\n",
    "            ret_test = cumulative_log_returns(price.iloc[s2:e2], h)\n",
    "            idx_test_common = out_test_raw[h].index.intersection(ret_test.index)\n",
    "            dfT = out_test_raw[h].loc[idx_test_common].copy()\n",
    "\n",
    "            maskT = np.isfinite(dfT[\"p_edge_raw\"].values) & np.isfinite(dfT[\"mu\"].values)\n",
    "            dfT = dfT[maskT]\n",
    "\n",
    "            if len(dfT) == 0:\n",
    "                continue\n",
    "\n",
    "            dfT[\"p_edge\"] = apply_prob_calibrator(cal_prob, dfT[\"p_edge_raw\"].values)\n",
    "            mu_test = dfT[\"mu\"].values\n",
    "            lo, hi = apply_conformal_interval(cal_pi, mu_test)\n",
    "            dfT[\"ret_lo\"] = lo\n",
    "            dfT[\"ret_hi\"] = hi\n",
    "\n",
    "            p_now = price.loc[dfT.index].values\n",
    "            dfT[\"price_lo\"] = p_now * np.exp(lo)\n",
    "            dfT[\"price_hi\"] = p_now * np.exp(hi)\n",
    "\n",
    "            dfT[\"edge\"] = dfT[\"mu\"] - cost_log[h]\n",
    "            dfT[\"risk_edge\"] = (dfT[\"mu\"] - cost_log[h]) / (dfT[\"std\"] + 1e-12)\n",
    "\n",
    "            results[h][\"test\"].append(dfT)\n",
    "\n",
    "            # Diagnostics\n",
    "            if cal_prob.method == \"isotonic\":\n",
    "                p_cal_val = apply_prob_calibrator(cal_prob, p_raw_val)\n",
    "                brier = brier_score(y_val, p_cal_val)\n",
    "                ece = expected_calibration_error(y_val, p_cal_val)\n",
    "            else:\n",
    "                brier = brier_score(y_val, p_raw_val)\n",
    "                ece = expected_calibration_error(y_val, p_raw_val)\n",
    "\n",
    "            coverage = float(np.mean((resid_val >= -cal_pi.q_alpha) & (resid_val <= cal_pi.q_alpha)))\n",
    "\n",
    "            diag = {\n",
    "                \"h\": h,\n",
    "                \"brier_val\": float(brier),\n",
    "                \"ece_val\": float(ece),\n",
    "                \"pi_coverage_val\": coverage\n",
    "            }\n",
    "            results[h][\"diag\"].append(diag)\n",
    "\n",
    "            # Store validation\n",
    "            dfV[\"p_edge\"] = apply_prob_calibrator(cal_prob, dfV[\"p_edge_raw\"].values)\n",
    "            loV, hiV = apply_conformal_interval(cal_pi, mu_val)\n",
    "            dfV[\"ret_lo\"], dfV[\"ret_hi\"] = loV, hiV\n",
    "            results[h][\"val\"].append(dfV)\n",
    "\n",
    "    # Concatenate folds\n",
    "    for h in horizons:\n",
    "        for split in (\"val\", \"test\"):\n",
    "            if results[h][split]:\n",
    "                results[h][split] = pd.concat(results[h][split]).sort_index()\n",
    "            else:\n",
    "                results[h][split] = pd.DataFrame()\n",
    "\n",
    "    print(f\"\\\\nCompleted {symbol}\\\\n\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509ca052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n============================================================\n",
      "Training SVM for BTC\n",
      "Model type: classification\n",
      "Horizons: [1, 3, 6]\n",
      "Folds: 3\n",
      "============================================================\\n\n",
      "Fold 1/3: Train[0:1749] Val[1773:3522] Test[3546:5295]\n",
      "  Training h=1... ✓\n",
      "  Training h=3... ✓\n",
      "  Training h=6... ✓\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "Fold 2/3: Train[0:3498] Val[3522:5271] Test[5295:7044]\n",
      "  Training h=1... ✓\n",
      "  Training h=3... ✓\n",
      "  Training h=6... ✓\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "Fold 3/3: Train[0:5247] Val[5271:7020] Test[7044:8747]\n",
      "  Training h=1... ✓\n",
      "  Training h=3... ✓\n",
      "  Training h=6... ✓\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "\\nCompleted BTC\\n\n",
      "\\n============================================================\n",
      "RESULTS FOR BTC\n",
      "============================================================\n",
      "\\nHorizon 1:\n",
      "  Test samples: 5198\n",
      "  Mean p_edge: 0.458\n",
      "  Mean mu: -0.0012\n",
      "  Avg Brier: 0.2450\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 79.98%\n",
      "\\nHorizon 3:\n",
      "  Test samples: 5192\n",
      "  Mean p_edge: 0.469\n",
      "  Mean mu: -0.0012\n",
      "  Avg Brier: 0.2463\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 80.01%\n",
      "\\nHorizon 6:\n",
      "  Test samples: 5183\n",
      "  Mean p_edge: 0.487\n",
      "  Mean mu: -0.0019\n",
      "  Avg Brier: 0.2472\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 79.98%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Run for one symbol\n",
    "    symbol = \"BTC\"\n",
    "    results = run_partA_for_symbol(\n",
    "        symbol=symbol,\n",
    "        path=FILES[symbol],\n",
    "        horizons=HORIZONS,\n",
    "        n_folds=3,\n",
    "        embargo=24,\n",
    "        model_type='classification',  # or 'regression'\n",
    "        n_bootstrap=0  # Set to 50 for uncertainty estimates\n",
    "    )\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\\\n{'='*60}\")\n",
    "    print(f\"RESULTS FOR {symbol}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for h in HORIZONS:\n",
    "        test_df = results[h]['test']\n",
    "        diag_list = results[h]['diag']\n",
    "\n",
    "        if len(test_df) > 0:\n",
    "            print(f\"\\\\nHorizon {h}:\")\n",
    "            print(f\"  Test samples: {len(test_df)}\")\n",
    "            print(f\"  Mean p_edge: {test_df['p_edge'].mean():.3f}\")\n",
    "            print(f\"  Mean mu: {test_df['mu'].mean():.4f}\")\n",
    "\n",
    "            if diag_list:\n",
    "                avg_brier = np.mean([d['brier_val'] for d in diag_list])\n",
    "                avg_ece = np.mean([d['ece_val'] for d in diag_list])\n",
    "                avg_cov = np.mean([d['pi_coverage_val'] for d in diag_list])\n",
    "                print(f\"  Avg Brier: {avg_brier:.4f}\")\n",
    "                print(f\"  Avg ECE: {avg_ece:.4f}\")\n",
    "                print(f\"  Avg PI coverage: {avg_cov:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd6184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training SVM for BTC\n",
      "Model type: classification\n",
      "Horizons: [1, 3, 6]\n",
      "Folds: 3\n",
      "============================================================\n",
      "\n",
      "Fold 1/3: Train[0:1749] Val[1773:3522] Test[3546:5295]\n",
      "  Training h=1... ✓\n",
      "  Training h=3... ✓\n",
      "  Training h=6... ✓\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "Fold 2/3: Train[0:3498] Val[3522:5271] Test[5295:7044]\n",
      "  Training h=1... ✓\n",
      "  Training h=3... ✓\n",
      "  Training h=6... ✓\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "Fold 3/3: Train[0:5247] Val[5271:7020] Test[7044:8747]\n",
      "  Training h=1... ✓\n",
      "  Training h=3... ✓\n",
      "  Training h=6... ✓\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "\n",
      "Completed BTC\n",
      "\n",
      "\n",
      "============================================================\n",
      "RESULTS FOR BTC\n",
      "============================================================\n",
      "\n",
      "Horizon 1:\n",
      "  Test samples: 5198\n",
      "  Mean p_edge: 0.458\n",
      "  Mean mu: 0.0002\n",
      "  Avg Brier: 0.2450\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 79.98%\n",
      "\n",
      "Horizon 3:\n",
      "  Test samples: 5192\n",
      "  Mean p_edge: 0.469\n",
      "  Mean mu: 0.0003\n",
      "  Avg Brier: 0.2463\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 80.01%\n",
      "\n",
      "Horizon 6:\n",
      "  Test samples: 5183\n",
      "  Mean p_edge: 0.487\n",
      "  Mean mu: 0.0002\n",
      "  Avg Brier: 0.2472\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 79.98%\n",
      "\n",
      "============================================================\n",
      "Training SVM for ETH\n",
      "Model type: classification\n",
      "Horizons: [1, 3, 6]\n",
      "Folds: 3\n",
      "============================================================\n",
      "\n",
      "Fold 1/3: Train[0:1749] Val[1773:3522] Test[3546:5295]\n",
      "  Training h=1... ✓\n",
      "  Training h=3... ✓\n",
      "  Training h=6... ✓\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "Fold 2/3: Train[0:3498] Val[3522:5271] Test[5295:7044]\n",
      "  Training h=1... ✓\n",
      "  Training h=3... ✓\n",
      "  Training h=6... ✓\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "Fold 3/3: Train[0:5247] Val[5271:7020] Test[7044:8747]\n",
      "  Training h=1... ✓\n",
      "  Training h=3... ✓\n",
      "  Training h=6... ✓\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "\n",
      "Completed ETH\n",
      "\n",
      "\n",
      "============================================================\n",
      "RESULTS FOR ETH\n",
      "============================================================\n",
      "\n",
      "Horizon 1:\n",
      "  Test samples: 5198\n",
      "  Mean p_edge: 0.465\n",
      "  Mean mu: 0.0003\n",
      "  Avg Brier: 0.2460\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 79.98%\n",
      "\n",
      "Horizon 3:\n",
      "  Test samples: 5192\n",
      "  Mean p_edge: 0.472\n",
      "  Mean mu: 0.0003\n",
      "  Avg Brier: 0.2464\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 80.01%\n",
      "\n",
      "Horizon 6:\n",
      "  Test samples: 5183\n",
      "  Mean p_edge: 0.487\n",
      "  Mean mu: 0.0004\n",
      "  Avg Brier: 0.2488\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 79.98%\n",
      "\n",
      "============================================================\n",
      "Training SVM for SOL\n",
      "Model type: classification\n",
      "Horizons: [1, 3, 6]\n",
      "Folds: 3\n",
      "============================================================\n",
      "\n",
      "Fold 1/3: Train[0:1749] Val[1773:3522] Test[3546:5295]\n",
      "  Training h=1... ✓\n",
      "  Training h=3... ✓\n",
      "  Training h=6... ✓\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "Fold 2/3: Train[0:3498] Val[3522:5271] Test[5295:7044]\n",
      "  Training h=1... ✓\n",
      "  Training h=3... ✓\n",
      "  Training h=6... ✓\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "Fold 3/3: Train[0:5247] Val[5271:7020] Test[7044:8747]\n",
      "  Training h=1... ✓\n",
      "  Training h=3... ✓\n",
      "  Training h=6... ✓\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "\n",
      "Completed SOL\n",
      "\n",
      "\n",
      "============================================================\n",
      "RESULTS FOR SOL\n",
      "============================================================\n",
      "\n",
      "Horizon 1:\n",
      "  Test samples: 5198\n",
      "  Mean p_edge: 0.485\n",
      "  Mean mu: 0.0003\n",
      "  Avg Brier: 0.2480\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 79.98%\n",
      "\n",
      "Horizon 3:\n",
      "  Test samples: 5192\n",
      "  Mean p_edge: 0.483\n",
      "  Mean mu: 0.0003\n",
      "  Avg Brier: 0.2477\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 80.01%\n",
      "\n",
      "Horizon 6:\n",
      "  Test samples: 5183\n",
      "  Mean p_edge: 0.484\n",
      "  Mean mu: 0.0002\n",
      "  Avg Brier: 0.2473\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 79.98%\n",
      "\n",
      "============================================================\n",
      "Training SVM for XRP\n",
      "Model type: classification\n",
      "Horizons: [1, 3, 6]\n",
      "Folds: 3\n",
      "============================================================\n",
      "\n",
      "Fold 1/3: Train[0:1749] Val[1773:3522] Test[3546:5295]\n",
      "  Training h=1... ✓\n",
      "  Training h=3... ✓\n",
      "  Training h=6... ✓\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "Fold 2/3: Train[0:3498] Val[3522:5271] Test[5295:7044]\n",
      "  Training h=1... ✓\n",
      "  Training h=3... ✓\n",
      "  Training h=6... ✓\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "Fold 3/3: Train[0:5247] Val[5271:7020] Test[7044:8747]\n",
      "  Training h=1... ✓\n",
      "  Training h=3... ✓\n",
      "  Training h=6... ✓\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "\n",
      "Completed XRP\n",
      "\n",
      "\n",
      "============================================================\n",
      "RESULTS FOR XRP\n",
      "============================================================\n",
      "\n",
      "Horizon 1:\n",
      "  Test samples: 5198\n",
      "  Mean p_edge: 0.472\n",
      "  Mean mu: 0.0004\n",
      "  Avg Brier: 0.2460\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 79.98%\n",
      "\n",
      "Horizon 3:\n",
      "  Test samples: 5192\n",
      "  Mean p_edge: 0.480\n",
      "  Mean mu: 0.0003\n",
      "  Avg Brier: 0.2474\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 80.01%\n",
      "\n",
      "Horizon 6:\n",
      "  Test samples: 5183\n",
      "  Mean p_edge: 0.481\n",
      "  Mean mu: 0.0003\n",
      "  Avg Brier: 0.2466\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 79.98%\n",
      "\n",
      "============================================================\n",
      "Training SVM for DOGE\n",
      "Model type: classification\n",
      "Horizons: [1, 3, 6]\n",
      "Folds: 3\n",
      "============================================================\n",
      "\n",
      "Fold 1/3: Train[0:1749] Val[1773:3522] Test[3546:5295]\n",
      "  Training h=1... ✓\n",
      "  Training h=3... ✓\n",
      "  Training h=6... ✓\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "Fold 2/3: Train[0:3498] Val[3522:5271] Test[5295:7044]\n",
      "  Training h=1... ✓\n",
      "  Training h=3... ✓\n",
      "  Training h=6... ✓\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "Fold 3/3: Train[0:5247] Val[5271:7020] Test[7044:8747]\n",
      "  Training h=1... ✓\n",
      "  Training h=3... ✓\n",
      "  Training h=6... ✓\n",
      "  Forecasting validation... ✓\n",
      "  Forecasting test... ✓\n",
      "\n",
      "Completed DOGE\n",
      "\n",
      "\n",
      "============================================================\n",
      "RESULTS FOR DOGE\n",
      "============================================================\n",
      "\n",
      "Horizon 1:\n",
      "  Test samples: 5198\n",
      "  Mean p_edge: 0.481\n",
      "  Mean mu: 0.0003\n",
      "  Avg Brier: 0.2457\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 79.98%\n",
      "\n",
      "Horizon 3:\n",
      "  Test samples: 5192\n",
      "  Mean p_edge: 0.487\n",
      "  Mean mu: 0.0003\n",
      "  Avg Brier: 0.2478\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 80.01%\n",
      "\n",
      "Horizon 6:\n",
      "  Test samples: 5183\n",
      "  Mean p_edge: 0.492\n",
      "  Mean mu: 0.0002\n",
      "  Avg Brier: 0.2476\n",
      "  Avg ECE: 0.0000\n",
      "  Avg PI coverage: 79.98%\n",
      "\n",
      "============================================================\n",
      "Wrote 77865 records to: /Users/nitinlodha/Desktop/ML/ML_Project/Bybit_CSV_Data/trader_feed_svm_multiH.jsonl\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "SVM Multi-Horizon Trading Signal Generator\n",
    "Complete implementation for cryptocurrency trading with SVM classification/regression\n",
    "\n",
    "This replaces the HMM-based approach with Support Vector Machines.\n",
    "Key features:\n",
    "- One SVM model per horizon (1, 3, 6 bars)\n",
    "- Calibrated probabilities for edge detection\n",
    "- Walk-forward cross-validation\n",
    "- Isotonic + conformal calibration\n",
    "- JSON feed output for trading agent\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# CONFIGURATION\n",
    "# =========================================\n",
    "DATA_DIR = Path(\"/Users/nitinlodha/Desktop/ML/ML_Project/Bybit_CSV_Data\")\n",
    "FILES = {\n",
    "    \"BTC\": DATA_DIR / \"Bybit_BTC.csv\",\n",
    "    \"ETH\": DATA_DIR / \"Bybit_ETH.csv\",\n",
    "    \"SOL\": DATA_DIR / \"Bybit_SOL.csv\",\n",
    "    \"XRP\": DATA_DIR / \"Bybit_XRP.csv\",\n",
    "    \"DOGE\": DATA_DIR / \"Bybit_DOGE.csv\",\n",
    "}\n",
    "\n",
    "HORIZONS = [1, 3, 6]  # Forecast horizons in 4-hour bars\n",
    "DEFAULT_COST_BP = {1: 8.0, 3: 10.0, 6: 12.0}  # Trading costs in basis points\n",
    "\n",
    "# Policy thresholds\n",
    "TAU_P = 0.60        # Probability gate for P(edge > cost)\n",
    "TAU_MU = 0.0005     # Expected-return gate (log-return)\n",
    "LAM = 2.0           # Kelly-lite multiplier\n",
    "W_MAX = 0.50        # Max gross position (50% notional)\n",
    "\n",
    "MODEL_VERSION = \"svm_rbf_multiH_v1.0\"\n",
    "CALIBRATION_VERSION = \"iso+conformal_v1\"\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =========================================\n",
    "def bp_to_logret(bp: float) -> float:\n",
    "    \"\"\"Convert basis points to log-return units.\"\"\"\n",
    "    return bp * 1e-4\n",
    "\n",
    "\n",
    "def _find_close_column(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Find the close price column in a dataframe.\"\"\"\n",
    "    lower = {c.lower(): c for c in df.columns}\n",
    "    for key in (\"close\", \"closing_price\", \"close_price\", \"price_close\", \"last\", \"c\"):\n",
    "        if key in lower:\n",
    "            return lower[key]\n",
    "    # Fallback: any single float column\n",
    "    float_cols = [c for c in df.columns if pd.api.types.is_float_dtype(df[c])]\n",
    "    if len(float_cols) == 1:\n",
    "        return float_cols[0]\n",
    "    raise ValueError(\"Cannot identify 'close' column.\")\n",
    "\n",
    "\n",
    "def cumulative_log_returns(price: pd.Series, h: int) -> pd.Series:\n",
    "    \"\"\"Compute log(P_{t+h}/P_t) aligned to t.\"\"\"\n",
    "    return np.log(price.shift(-h) / price).dropna()\n",
    "\n",
    "\n",
    "def brier_score(y: np.ndarray, p: np.ndarray) -> float:\n",
    "    \"\"\"Brier score for probability calibration.\"\"\"\n",
    "    return float(np.mean((y - p) ** 2))\n",
    "\n",
    "\n",
    "def expected_calibration_error(y: np.ndarray, p: np.ndarray, bins: int = 10) -> float:\n",
    "    \"\"\"Expected Calibration Error (ECE).\"\"\"\n",
    "    edges = np.linspace(0, 1, bins + 1)\n",
    "    ece = 0.0\n",
    "    for i in range(bins):\n",
    "        m = (p >= edges[i]) & (p < edges[i+1])\n",
    "        if m.sum() == 0:\n",
    "            continue\n",
    "        ece += (m.sum()/len(p)) * np.abs(np.mean(y[m]) - np.mean(p[m]))\n",
    "    return float(ece)\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# FEATURE ENGINEERING\n",
    "# =========================================\n",
    "def make_feature_table(close: pd.Series):\n",
    "    \"\"\"\n",
    "    Build feature table from close prices.\n",
    "\n",
    "    Features:\n",
    "    - Returns at multiple lags (1, 3, 6 bars)\n",
    "    - Rolling volatility (6, 12 bars)\n",
    "    - Moving average ratio (log MA10/MA20)\n",
    "\n",
    "    Returns:\n",
    "        df: DataFrame with price and features\n",
    "        X: Feature matrix (numpy array)\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(index=close.index)\n",
    "    df[\"price\"] = close.astype(float)\n",
    "\n",
    "    # Log returns\n",
    "    df[\"ret_1\"] = np.log(df[\"price\"] / df[\"price\"].shift(1))\n",
    "    df[\"ret_3\"] = np.log(df[\"price\"] / df[\"price\"].shift(3))\n",
    "    df[\"ret_6\"] = np.log(df[\"price\"] / df[\"price\"].shift(6))\n",
    "\n",
    "    # Volatility\n",
    "    df[\"vol_6\"] = df[\"ret_1\"].rolling(6).std()\n",
    "    df[\"vol_12\"] = df[\"ret_1\"].rolling(12).std()\n",
    "\n",
    "    # Moving average ratio\n",
    "    ma_10 = df[\"price\"].rolling(10).mean()\n",
    "    ma_20 = df[\"price\"].rolling(20).mean()\n",
    "    df[\"ma_ratio\"] = np.log(ma_10 / ma_20)\n",
    "\n",
    "    # Drop NaN rows\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Feature matrix (exclude price)\n",
    "    feat_cols = [c for c in df.columns if c != \"price\"]\n",
    "    X = df[feat_cols].values.astype(float)\n",
    "\n",
    "    return df, X\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# WALK-FORWARD CV\n",
    "# =========================================\n",
    "def purged_walkforward_slices(n: int, n_folds: int = 3, embargo: int = 24):\n",
    "    \"\"\"\n",
    "    Generate (train, val, test) slices for walk-forward CV with embargo.\n",
    "\n",
    "    Args:\n",
    "        n: Total number of samples\n",
    "        n_folds: Number of folds\n",
    "        embargo: Gap between train/val and val/test (in bars)\n",
    "\n",
    "    Returns:\n",
    "        List of ((train_start, train_end), (val_start, val_end), (test_start, test_end))\n",
    "    \"\"\"\n",
    "    fold_size = n // (n_folds + 2)\n",
    "    slices = []\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        train_end = (i + 1) * fold_size\n",
    "        val_start = train_end + embargo\n",
    "        val_end = val_start + fold_size\n",
    "        test_start = val_end + embargo\n",
    "        test_end = min(test_start + fold_size, n)\n",
    "\n",
    "        if test_end - test_start < fold_size // 2:\n",
    "            break\n",
    "\n",
    "        slices.append((\n",
    "            (0, train_end),\n",
    "            (val_start, val_end),\n",
    "            (test_start, test_end)\n",
    "        ))\n",
    "\n",
    "    return slices\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# SVM MODEL\n",
    "# =========================================\n",
    "@dataclass\n",
    "class SVMSnapshot:\n",
    "    \"\"\"Container for trained SVM model and preprocessing.\"\"\"\n",
    "    clf: SVC | SVR\n",
    "    scaler: StandardScaler\n",
    "    model_type: str  # 'classification' or 'regression'\n",
    "    horizon: int\n",
    "    C: float = 1.0\n",
    "    gamma: float | str = 'scale'\n",
    "\n",
    "\n",
    "def fit_svm_classifier(X_train: np.ndarray, y_train: np.ndarray,\n",
    "                       horizon: int, random_state: int = 123,\n",
    "                       C: float = 10.0, gamma: float | str = 'scale') -> SVMSnapshot:\n",
    "    \"\"\"\n",
    "    Train calibrated SVM classifier for binary edge detection.\n",
    "\n",
    "    Args:\n",
    "        X_train: Feature matrix (T, D)\n",
    "        y_train: Binary labels (1 if return > cost, else 0)\n",
    "        horizon: Forecast horizon in bars\n",
    "        C: SVM regularization parameter (higher = less regularization)\n",
    "        gamma: RBF kernel parameter ('scale', 'auto', or float)\n",
    "\n",
    "    Returns:\n",
    "        SVMSnapshot with calibrated classifier\n",
    "    \"\"\"\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_scaled = scaler.transform(X_train)\n",
    "\n",
    "    # Base SVM with RBF kernel\n",
    "    base_svm = SVC(\n",
    "        kernel='rbf',\n",
    "        C=C,\n",
    "        gamma=gamma,\n",
    "        class_weight='balanced',  # Handle imbalanced classes\n",
    "        random_state=random_state,\n",
    "        probability=False,  # Calibration provides probabilities\n",
    "        cache_size=500  # MB for kernel cache\n",
    "    )\n",
    "\n",
    "    # Calibrate probabilities using isotonic regression\n",
    "    # This is CRITICAL for reliable P(edge > cost) estimates\n",
    "    clf = CalibratedClassifierCV(\n",
    "        base_svm,\n",
    "        method='isotonic',\n",
    "        cv=3,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    clf.fit(X_scaled, y_train)\n",
    "\n",
    "    return SVMSnapshot(\n",
    "        clf=clf,\n",
    "        scaler=scaler,\n",
    "        model_type='classification',\n",
    "        horizon=horizon,\n",
    "        C=C,\n",
    "        gamma=gamma\n",
    "    )\n",
    "\n",
    "\n",
    "def fit_svm_regressor(X_train: np.ndarray, y_train: np.ndarray,\n",
    "                      horizon: int, random_state: int = 123,\n",
    "                      C: float = 10.0, gamma: float | str = 'scale',\n",
    "                      epsilon: float = 0.01) -> SVMSnapshot:\n",
    "    \"\"\"\n",
    "    Train SVR to directly predict continuous returns.\n",
    "\n",
    "    Args:\n",
    "        X_train: Feature matrix\n",
    "        y_train: Continuous log returns\n",
    "        epsilon: Epsilon-insensitive loss parameter\n",
    "\n",
    "    Returns:\n",
    "        SVMSnapshot with trained regressor\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_scaled = scaler.transform(X_train)\n",
    "\n",
    "    reg = SVR(\n",
    "        kernel='rbf',\n",
    "        C=C,\n",
    "        gamma=gamma,\n",
    "        epsilon=epsilon,\n",
    "        cache_size=500\n",
    "    )\n",
    "\n",
    "    reg.fit(X_scaled, y_train)\n",
    "\n",
    "    return SVMSnapshot(\n",
    "        clf=reg,\n",
    "        scaler=scaler,\n",
    "        model_type='regression',\n",
    "        horizon=horizon,\n",
    "        C=C,\n",
    "        gamma=gamma\n",
    "    )\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# FORECASTING\n",
    "# =========================================\n",
    "def forecast_multi_horizon_svm(\n",
    "    snapshots: dict[int, SVMSnapshot],\n",
    "    X_seg: np.ndarray,\n",
    "    price_seg: pd.Series,\n",
    "    horizons: list[int],\n",
    "    cost_bp: dict[int, float] | None = None,\n",
    "    n_bootstrap: int = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate multi-horizon forecasts using trained SVMs.\n",
    "\n",
    "    Args:\n",
    "        snapshots: Dict mapping horizon -> trained SVMSnapshot\n",
    "        X_seg: Feature matrix for forecast segment\n",
    "        price_seg: Corresponding price series\n",
    "        horizons: List of forecast horizons\n",
    "        cost_bp: Trading costs in basis points\n",
    "        n_bootstrap: Bootstrap samples for uncertainty (0 = disabled)\n",
    "\n",
    "    Returns:\n",
    "        out: dict[horizon] -> DataFrame with predictions\n",
    "        cost_log: dict[horizon] -> cost in log-return units\n",
    "    \"\"\"\n",
    "    if cost_bp is None:\n",
    "        cost_bp = {h: DEFAULT_COST_BP.get(h, 8.0) for h in horizons}\n",
    "    cost_log = {h: bp_to_logret(float(cost_bp[h])) for h in horizons}\n",
    "\n",
    "    Tseg = X_seg.shape[0]\n",
    "    idx = price_seg.index\n",
    "    out = {}\n",
    "\n",
    "    for h in horizons:\n",
    "        if h not in snapshots:\n",
    "            print(f\"Warning: No model for horizon {h}, skipping\")\n",
    "            continue\n",
    "\n",
    "        snap = snapshots[h]\n",
    "        out_h = pd.DataFrame(index=idx[:-h] if h < Tseg else idx[:0])\n",
    "        T_h = Tseg - h\n",
    "\n",
    "        if T_h <= 0:\n",
    "            out[h] = out_h\n",
    "            continue\n",
    "\n",
    "        # Scale features\n",
    "        X_scaled = snap.scaler.transform(X_seg[:T_h])\n",
    "\n",
    "        if snap.model_type == 'classification':\n",
    "            # Get calibrated probabilities\n",
    "            p_edge = snap.clf.predict_proba(X_scaled)[:, 1]\n",
    "\n",
    "            # Expected return estimation (probability-weighted)\n",
    "            mu = p_edge * (cost_log[h] + 0.002) + (1 - p_edge) * (-cost_log[h] - 0.001)\n",
    "\n",
    "            # Uncertainty\n",
    "            if n_bootstrap > 0:\n",
    "                std_h, q10, q50, q90 = _bootstrap_uncertainty_classification(\n",
    "                    snap, X_scaled, n_bootstrap, cost_log[h]\n",
    "                )\n",
    "            else:\n",
    "                std_h = np.full(T_h, 0.01)\n",
    "                q10 = mu - 0.02\n",
    "                q50 = mu\n",
    "                q90 = mu + 0.02\n",
    "\n",
    "        elif snap.model_type == 'regression':\n",
    "            # Direct return prediction\n",
    "            mu = snap.clf.predict(X_scaled)\n",
    "\n",
    "            # Probability via sigmoid transform\n",
    "            p_edge = 1.0 / (1.0 + np.exp(-10 * (mu - cost_log[h])))\n",
    "\n",
    "            if n_bootstrap > 0:\n",
    "                std_h, q10, q50, q90 = _bootstrap_uncertainty_regression(\n",
    "                    snap, X_scaled, n_bootstrap\n",
    "                )\n",
    "            else:\n",
    "                std_h = np.full(T_h, 0.015)\n",
    "                q10 = mu - 0.025\n",
    "                q50 = mu\n",
    "                q90 = mu + 0.025\n",
    "\n",
    "        # Populate DataFrame\n",
    "        p_now = price_seg.iloc[:T_h].values\n",
    "\n",
    "        out_h['mu'] = mu\n",
    "        out_h['std'] = std_h\n",
    "        out_h['p_edge_raw'] = p_edge\n",
    "        out_h['ret_q10'] = q10\n",
    "        out_h['ret_q50'] = q50\n",
    "        out_h['ret_q90'] = q90\n",
    "        out_h['price_pred'] = p_now * np.exp(mu)\n",
    "        out_h['price_q10'] = p_now * np.exp(q10)\n",
    "        out_h['price_q50'] = p_now * np.exp(q50)\n",
    "        out_h['price_q90'] = p_now * np.exp(q90)\n",
    "\n",
    "        out[h] = out_h\n",
    "\n",
    "    return out, cost_log\n",
    "\n",
    "\n",
    "def _bootstrap_uncertainty_classification(snap, X_scaled, n_bootstrap, cost_threshold):\n",
    "    \"\"\"Bootstrap resampling for classification uncertainty.\"\"\"\n",
    "    T = len(X_scaled)\n",
    "    rng = np.random.default_rng(42)\n",
    "    probs = []\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        boot_idx = rng.choice(T, size=T, replace=True)\n",
    "        X_boot = X_scaled[boot_idx]\n",
    "        p_boot = snap.clf.predict_proba(X_boot)[:, 1]\n",
    "        probs.append(p_boot[:T])\n",
    "\n",
    "    probs = np.array(probs)\n",
    "    std = np.std(probs, axis=0)\n",
    "\n",
    "    # Convert probabilities to return estimates\n",
    "    mu_samples = probs * (cost_threshold + 0.002) + (1 - probs) * (-cost_threshold - 0.001)\n",
    "\n",
    "    q10 = np.percentile(mu_samples, 10, axis=0)\n",
    "    q50 = np.percentile(mu_samples, 50, axis=0)\n",
    "    q90 = np.percentile(mu_samples, 90, axis=0)\n",
    "\n",
    "    return std, q10, q50, q90\n",
    "\n",
    "\n",
    "def _bootstrap_uncertainty_regression(snap, X_scaled, n_bootstrap):\n",
    "    \"\"\"Bootstrap resampling for regression uncertainty.\"\"\"\n",
    "    T = len(X_scaled)\n",
    "    rng = np.random.default_rng(42)\n",
    "    preds = []\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        boot_idx = rng.choice(T, size=T, replace=True)\n",
    "        X_boot = X_scaled[boot_idx]\n",
    "        pred_boot = snap.clf.predict(X_boot)\n",
    "        preds.append(pred_boot[:T])\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    std = np.std(preds, axis=0)\n",
    "    q10 = np.percentile(preds, 10, axis=0)\n",
    "    q50 = np.percentile(preds, 50, axis=0)\n",
    "    q90 = np.percentile(preds, 90, axis=0)\n",
    "\n",
    "    return std, q10, q50, q90\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# CALIBRATION\n",
    "# =========================================\n",
    "@dataclass\n",
    "class ProbCalibrator:\n",
    "    \"\"\"Probability calibrator using isotonic regression.\"\"\"\n",
    "    method: str\n",
    "    iso: IsotonicRegression | None = None\n",
    "\n",
    "\n",
    "def fit_prob_calibrator_isotonic(p_raw: np.ndarray, y: np.ndarray,\n",
    "                                 min_points: int = 30) -> ProbCalibrator:\n",
    "    \"\"\"Fit isotonic regression p_raw -> y.\"\"\"\n",
    "    p_raw = np.asarray(p_raw, float)\n",
    "    y = np.asarray(y, float)\n",
    "    m = np.isfinite(p_raw) & np.isfinite(y)\n",
    "    p, t = p_raw[m], y[m]\n",
    "    if p.size < min_points or np.unique(p).size < 3:\n",
    "        return ProbCalibrator(method=\"identity\", iso=None)\n",
    "    iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "    iso.fit(p, t)\n",
    "    return ProbCalibrator(method=\"isotonic\", iso=iso)\n",
    "\n",
    "\n",
    "def apply_prob_calibrator(cal: ProbCalibrator, p_raw: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Apply probability calibrator.\"\"\"\n",
    "    p_raw = np.asarray(p_raw, float)\n",
    "    if cal.method == \"isotonic\":\n",
    "        return cal.iso.predict(p_raw)\n",
    "    return p_raw\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class IntervalCalibrator:\n",
    "    \"\"\"Conformal prediction interval calibrator.\"\"\"\n",
    "    method: str\n",
    "    q_alpha: float\n",
    "    alpha: float\n",
    "\n",
    "\n",
    "def fit_conformal_interval(residuals: np.ndarray, alpha: float = 0.2) -> IntervalCalibrator:\n",
    "    \"\"\"Fit conformal prediction intervals.\"\"\"\n",
    "    resid = np.asarray(residuals, float)\n",
    "    resid = resid[np.isfinite(resid)]\n",
    "    q = float(np.quantile(np.abs(resid), 1 - alpha)) if resid.size > 0 else 0.0\n",
    "    return IntervalCalibrator(method=\"conformal_abs\", q_alpha=q, alpha=alpha)\n",
    "\n",
    "\n",
    "def apply_conformal_interval(cal: IntervalCalibrator, mu: np.ndarray):\n",
    "    \"\"\"Apply conformal prediction intervals.\"\"\"\n",
    "    mu = np.asarray(mu, float)\n",
    "    return mu - cal.q_alpha, mu + cal.q_alpha\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# MAIN TRAINING PIPELINE\n",
    "# =========================================\n",
    "def run_svm_for_symbol(symbol: str, path: Path,\n",
    "                       horizons: list[int] = HORIZONS,\n",
    "                       n_folds: int = 3,\n",
    "                       embargo: int = 24,\n",
    "                       model_type: str = 'classification',\n",
    "                       n_bootstrap: int = 0):\n",
    "    \"\"\"\n",
    "    Train and evaluate SVM models for one symbol.\n",
    "\n",
    "    Args:\n",
    "        symbol: Asset symbol\n",
    "        path: Path to CSV file\n",
    "        horizons: Forecast horizons in bars\n",
    "        n_folds: Number of walk-forward folds\n",
    "        embargo: Embargo period between folds\n",
    "        model_type: 'classification' or 'regression'\n",
    "        n_bootstrap: Bootstrap samples for uncertainty (0 = disabled)\n",
    "\n",
    "    Returns:\n",
    "        results: dict[horizon] -> dict with 'val', 'test', 'diag' DataFrames\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df_raw = pd.read_csv(path)\n",
    "    close_col = _find_close_column(df_raw)\n",
    "    close = pd.Series(df_raw[close_col].astype(float).values,\n",
    "                      index=pd.RangeIndex(len(df_raw)), name=\"close\")\n",
    "\n",
    "    feat_df, X = make_feature_table(close)\n",
    "    price = feat_df[\"price\"]\n",
    "    n = len(price)\n",
    "\n",
    "    folds = purged_walkforward_slices(n, n_folds=n_folds, embargo=embargo)\n",
    "\n",
    "    results = {h: {\"val\": [], \"test\": [], \"diag\": []} for h in horizons}\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training SVM for {symbol}\")\n",
    "    print(f\"Model type: {model_type}\")\n",
    "    print(f\"Horizons: {horizons}\")\n",
    "    print(f\"Folds: {n_folds}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    for fold_idx, ((s0,e0), (s1,e1), (s2,e2)) in enumerate(folds):\n",
    "        print(f\"Fold {fold_idx + 1}/{len(folds)}: Train[{s0}:{e0}] Val[{s1}:{e1}] Test[{s2}:{e2}]\")\n",
    "\n",
    "        # Train one SVM per horizon\n",
    "        snapshots = {}\n",
    "\n",
    "        for h in horizons:\n",
    "            print(f\"  Training h={h}...\", end=\" \")\n",
    "\n",
    "            # Create labels for this horizon\n",
    "            ret_train = cumulative_log_returns(price.iloc[s0:e0], h)\n",
    "\n",
    "            # Align features and labels\n",
    "            n_train = min(len(X[s0:e0]), len(ret_train))\n",
    "            X_train_aligned = X[s0:s0+n_train]\n",
    "            ret_train_aligned = ret_train.iloc[:n_train]\n",
    "\n",
    "            if len(X_train_aligned) < 50:\n",
    "                print(\"SKIP (insufficient data)\")\n",
    "                continue\n",
    "\n",
    "            if model_type == 'classification':\n",
    "                # Binary classification\n",
    "                y_train = (ret_train_aligned.values > bp_to_logret(DEFAULT_COST_BP[h])).astype(int)\n",
    "\n",
    "                # Check class balance\n",
    "                pos_frac = y_train.mean()\n",
    "                if pos_frac < 0.1 or pos_frac > 0.9:\n",
    "                    print(f\"WARN (imbalanced: {pos_frac:.2%} positive)\")\n",
    "\n",
    "                snap = fit_svm_classifier(\n",
    "                    X_train_aligned, y_train,\n",
    "                    horizon=h,\n",
    "                    random_state=123 + fold_idx,\n",
    "                    C=10.0,\n",
    "                    gamma='scale'\n",
    "                )\n",
    "            else:\n",
    "                # Regression\n",
    "                y_train = ret_train_aligned.values\n",
    "                snap = fit_svm_regressor(\n",
    "                    X_train_aligned, y_train,\n",
    "                    horizon=h,\n",
    "                    random_state=123 + fold_idx,\n",
    "                    C=10.0,\n",
    "                    gamma='scale',\n",
    "                    epsilon=0.01\n",
    "                )\n",
    "\n",
    "            snapshots[h] = snap\n",
    "            print(\"✓\")\n",
    "\n",
    "        if not snapshots:\n",
    "            print(\"  No models trained, skipping fold\")\n",
    "            continue\n",
    "\n",
    "        # Forecast on validation and test\n",
    "        print(\"  Forecasting validation...\", end=\" \")\n",
    "        out_val_raw, cost_log = forecast_multi_horizon_svm(\n",
    "            snapshots=snapshots,\n",
    "            X_seg=X[s1:e1],\n",
    "            price_seg=price.iloc[s1:e1],\n",
    "            horizons=horizons,\n",
    "            n_bootstrap=n_bootstrap\n",
    "        )\n",
    "        print(\"✓\")\n",
    "\n",
    "        print(\"  Forecasting test...\", end=\" \")\n",
    "        out_test_raw, _ = forecast_multi_horizon_svm(\n",
    "            snapshots=snapshots,\n",
    "            X_seg=X[s2:e2],\n",
    "            price_seg=price.iloc[s2:e2],\n",
    "            horizons=horizons,\n",
    "            n_bootstrap=n_bootstrap\n",
    "        )\n",
    "        print(\"✓\")\n",
    "\n",
    "        # Calibration\n",
    "        for h in horizons:\n",
    "            if h not in out_val_raw or h not in out_test_raw:\n",
    "                continue\n",
    "\n",
    "            ret_val = cumulative_log_returns(price.iloc[s1:e1], h)\n",
    "            idx_common = out_val_raw[h].index.intersection(ret_val.index)\n",
    "\n",
    "            if len(idx_common) == 0:\n",
    "                continue\n",
    "\n",
    "            dfV = out_val_raw[h].loc[idx_common].copy()\n",
    "            maskV = np.isfinite(dfV[\"p_edge_raw\"].values) & np.isfinite(dfV[\"mu\"].values)\n",
    "            dfV = dfV[maskV]\n",
    "\n",
    "            if len(dfV) < 20:\n",
    "                continue\n",
    "\n",
    "            ret_val_aligned = ret_val.loc[dfV.index]\n",
    "            y_val = (ret_val_aligned.values > cost_log[h]).astype(int)\n",
    "            p_raw_val = dfV[\"p_edge_raw\"].values\n",
    "            mu_val = dfV[\"mu\"].values\n",
    "\n",
    "            # Fit calibrators\n",
    "            cal_prob = fit_prob_calibrator_isotonic(p_raw_val, y_val, min_points=20)\n",
    "            resid_val = ret_val_aligned.values - mu_val\n",
    "            cal_pi = fit_conformal_interval(resid_val, alpha=0.2)\n",
    "\n",
    "            # Apply to test\n",
    "            ret_test = cumulative_log_returns(price.iloc[s2:e2], h)\n",
    "            idx_test_common = out_test_raw[h].index.intersection(ret_test.index)\n",
    "            dfT = out_test_raw[h].loc[idx_test_common].copy()\n",
    "\n",
    "            maskT = np.isfinite(dfT[\"p_edge_raw\"].values) & np.isfinite(dfT[\"mu\"].values)\n",
    "            dfT = dfT[maskT]\n",
    "\n",
    "            if len(dfT) == 0:\n",
    "                continue\n",
    "\n",
    "            dfT[\"p_edge\"] = apply_prob_calibrator(cal_prob, dfT[\"p_edge_raw\"].values)\n",
    "            mu_test = dfT[\"mu\"].values\n",
    "            lo, hi = apply_conformal_interval(cal_pi, mu_test)\n",
    "            dfT[\"ret_lo\"] = lo\n",
    "            dfT[\"ret_hi\"] = hi\n",
    "\n",
    "            p_now = price.loc[dfT.index].values\n",
    "            dfT[\"price_lo\"] = p_now * np.exp(lo)\n",
    "            dfT[\"price_hi\"] = p_now * np.exp(hi)\n",
    "\n",
    "            dfT[\"edge\"] = dfT[\"mu\"] - cost_log[h]\n",
    "            dfT[\"risk_edge\"] = (dfT[\"mu\"] - cost_log[h]) / (dfT[\"std\"] + 1e-12)\n",
    "\n",
    "            results[h][\"test\"].append(dfT)\n",
    "\n",
    "            # Diagnostics\n",
    "            if cal_prob.method == \"isotonic\":\n",
    "                p_cal_val = apply_prob_calibrator(cal_prob, p_raw_val)\n",
    "                brier = brier_score(y_val, p_cal_val)\n",
    "                ece = expected_calibration_error(y_val, p_cal_val)\n",
    "            else:\n",
    "                brier = brier_score(y_val, p_raw_val)\n",
    "                ece = expected_calibration_error(y_val, p_raw_val)\n",
    "\n",
    "            coverage = float(np.mean((resid_val >= -cal_pi.q_alpha) & (resid_val <= cal_pi.q_alpha)))\n",
    "\n",
    "            diag = {\n",
    "                \"h\": h,\n",
    "                \"brier_val\": float(brier),\n",
    "                \"ece_val\": float(ece),\n",
    "                \"pi_coverage_val\": coverage\n",
    "            }\n",
    "            results[h][\"diag\"].append(diag)\n",
    "\n",
    "            # Store validation\n",
    "            dfV[\"p_edge\"] = apply_prob_calibrator(cal_prob, dfV[\"p_edge_raw\"].values)\n",
    "            loV, hiV = apply_conformal_interval(cal_pi, mu_val)\n",
    "            dfV[\"ret_lo\"], dfV[\"ret_hi\"] = loV, hiV\n",
    "            results[h][\"val\"].append(dfV)\n",
    "\n",
    "    # Concatenate folds\n",
    "    for h in horizons:\n",
    "        for split in (\"val\", \"test\"):\n",
    "            if results[h][split]:\n",
    "                results[h][split] = pd.concat(results[h][split]).sort_index()\n",
    "            else:\n",
    "                results[h][split] = pd.DataFrame()\n",
    "\n",
    "    print(f\"\\nCompleted {symbol}\\n\")\n",
    "    return results\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# JSON EXPORT\n",
    "# =========================================\n",
    "def build_json_records(all_outputs: dict,\n",
    "                       model_version: str = MODEL_VERSION,\n",
    "                       calibration_version: str = CALIBRATION_VERSION,\n",
    "                       horizons: list[int] = HORIZONS):\n",
    "    \"\"\"Build JSONL records for trading agent.\"\"\"\n",
    "    records = []\n",
    "    for sym, res in all_outputs.items():\n",
    "        for h in horizons:\n",
    "            df = res[h][\"test\"]\n",
    "            if isinstance(df, list) or isinstance(df, tuple):\n",
    "                df = pd.concat(df).sort_index()\n",
    "            for t, row in df.iterrows():\n",
    "                rec = {\n",
    "                    \"timestamp_index\": int(t),\n",
    "                    \"symbol\": sym,\n",
    "                    \"horizon_bars\": int(h),\n",
    "                    \"model_version\": model_version,\n",
    "                    \"calibration_version\": calibration_version,\n",
    "                    \"signals\": {\n",
    "                        \"expected_return\": float(row[\"mu\"]),\n",
    "                        \"stdev_return\": float(row[\"std\"]),\n",
    "                        \"p_edge_gt_cost\": float(row[\"p_edge\"]),\n",
    "                        \"predicted_price\": float(row[\"price_pred\"]),\n",
    "                        \"price_PI\": {\n",
    "                            \"p10\": float(row[\"price_q10\"]),\n",
    "                            \"p50\": float(row[\"price_q50\"]),\n",
    "                            \"p90\": float(row[\"price_q90\"])\n",
    "                        }\n",
    "                    },\n",
    "                    \"policy_suggestions\": {\n",
    "                        \"gate_threshold_p\": TAU_P,\n",
    "                        \"gate_threshold_edge\": TAU_MU,\n",
    "                        \"suggested_action\": \"buy\" if (row[\"p_edge\"]>=TAU_P and row[\"edge\"]>=TAU_MU and row[\"mu\"]>0)\n",
    "                                            else (\"sell\" if (row[\"p_edge\"]>=TAU_P and row[\"edge\"]>=TAU_MU and row[\"mu\"]<0)\n",
    "                                                  else \"hold\")\n",
    "                    }\n",
    "                }\n",
    "                records.append(rec)\n",
    "    return records\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# MAIN EXECUTION\n",
    "# =========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Process all symbols\n",
    "    all_outputs = {}\n",
    "\n",
    "    for symbol, path in FILES.items():\n",
    "        if not path.exists():\n",
    "            print(f\"Warning: {path} not found, skipping {symbol}\")\n",
    "            continue\n",
    "\n",
    "        results = run_svm_for_symbol(\n",
    "            symbol=symbol,\n",
    "            path=path,\n",
    "            horizons=HORIZONS,\n",
    "            n_folds=3,\n",
    "            embargo=24,\n",
    "            model_type='classification',  # or 'regression'\n",
    "            n_bootstrap=0  # Set to 50 for uncertainty estimates\n",
    "        )\n",
    "\n",
    "        all_outputs[symbol] = results\n",
    "\n",
    "        # Print summary\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"RESULTS FOR {symbol}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        for h in HORIZONS:\n",
    "            test_df = results[h]['test']\n",
    "            diag_list = results[h]['diag']\n",
    "\n",
    "            if len(test_df) > 0:\n",
    "                print(f\"\\nHorizon {h}:\")\n",
    "                print(f\"  Test samples: {len(test_df)}\")\n",
    "                print(f\"  Mean p_edge: {test_df['p_edge'].mean():.3f}\")\n",
    "                print(f\"  Mean mu: {test_df['mu'].mean():.4f}\")\n",
    "\n",
    "                if diag_list:\n",
    "                    avg_brier = np.mean([d['brier_val'] for d in diag_list])\n",
    "                    avg_ece = np.mean([d['ece_val'] for d in diag_list])\n",
    "                    avg_cov = np.mean([d['pi_coverage_val'] for d in diag_list])\n",
    "                    print(f\"  Avg Brier: {avg_brier:.4f}\")\n",
    "                    print(f\"  Avg ECE: {avg_ece:.4f}\")\n",
    "                    print(f\"  Avg PI coverage: {avg_cov:.2%}\")\n",
    "\n",
    "    # Export to JSON\n",
    "    json_records = build_json_records(all_outputs)\n",
    "    json_path = DATA_DIR / \"trader_feed_svm_multiH.jsonl\"\n",
    "    with open(json_path, \"w\") as f:\n",
    "        for r in json_records:\n",
    "            f.write(json.dumps(r) + \"\\n\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Wrote {len(json_records)} records to: {json_path}\")\n",
    "    print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3191f50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
